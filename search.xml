<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hardware Compute Partitioning on NVIDIA GPUs 论文笔记</title>
    <url>/2024/10/23/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>嵌入式和自主系统越来越多地集成 AI&#x2F;ML 功能，这些功能通常由 GPU 等硬件加速器启用。随着这些工作负载的要求越来越高，但尺寸、重量、功耗和成本限制仍然存在，迫切需要增加 GPU 容量的方法。在这项工作中，我们提供了一种透明地对 NVIDIA GPU 的计算单元进行<strong>空间分区</strong>的方法，从而允许通过安全高效的 GPU 共享来回收闲置的容量。我们的方法适用于 2013 年以来的任何 NVIDIA GPU，并且可以通过我们易于使用的名为 libsmctrl 的用户空间库来应用。我们通过深入研究 NVIDIA GPU 的<strong>硬件调度管道</strong>来支持我们的系统设计。我们提供了系统的使用指南，并通过使用 YOLOv2 的对象检测案例研究进行了演示。</p>
<span id="more"></span>



<h2 id="文章主体"><a href="#文章主体" class="headerlink" title="文章主体"></a>文章主体</h2><h3 id="主要创新点"><a href="#主要创新点" class="headerlink" title="主要创新点"></a>主要创新点</h3><h4 id="现有不足"><a href="#现有不足" class="headerlink" title="现有不足"></a>现有不足</h4><ul>
<li>当前的 GPU 管理和调度方法通常将 GPU 视为单个整体设备并强制执行互斥的访问控制。这可能会导致严重的容量损失，类似于在多核 CPU 上一次仅调度一项任务。</li>
<li>现有的关于nvidia-gpu的文档不足，各个方法都继承了协作多任务处理的许多问题，包括缺乏与行为不当的任务的隔离。这些方法还限制了允许的内核启动类型，导致不可避免的指令缓存干扰，并且除其他缺陷外，还需要大量的专家程序修改。</li>
</ul>
<h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><ul>
<li>揭示自 2013 年以来所有 NVIDIA GPU 中计算单元空间分区的硬件机制。 </li>
<li>构建并演示一个简单、有效且可移植的 GPU 空间分区  API。</li>
<li>提供迄今为止未公开的 NVIDIA GPU 硬件调度管道的详细信息。 </li>
<li>迄今为止未发布的 NVIDIA GPU  架构模式的详细信息，包括计算单元和内存单元之间的布局和互连。</li>
<li>提供高效的分区方针</li>
<li>通过案例研究演示如何应用空间分区来提高 GPU 利用率并减少卷积神经网络 (CNN) 的延迟</li>
</ul>
<h3 id="理论阐释"><a href="#理论阐释" class="headerlink" title="理论阐释"></a>理论阐释</h3><h4 id="硬件分区实现"><a href="#硬件分区实现" class="headerlink" title="硬件分区实现"></a>硬件分区实现</h4><p>内核 i 的第 j 个块表示为 Ki:j</p>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025131917706.png" alt="image-20241025131917706"></p>
<p>右边使用SM掩码，让K1无法使用5-8，K2无法使用0-5，可以让K2快速启动。</p>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025132031131.png" alt="image-20241025132031131"></p>
<p>不同型号的掩码分区效果不同：GTX 1060 和 Tesla P100 均基于 NVIDIA 的 Pascal 架构，但使用的芯片配置方式略有不同。 P100 和较新的 GPU 的配置与图 2 中的 GA100 更相似，每个 TPC 有两个 SM，而 GTX 1060 和较旧的 GPU 具有一对一的 SM 与 TPC 比率。这一关键区别和大量支持结果表明“<strong>SM 掩码”实际上充当 GPU 的 TPC 掩码</strong>。据推测，TMD中字段的名称继承自TPC和SM数量相同的时代，允许更多的术语互换。</p>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025132323178.png" alt="image-20241025132323178"></p>
<p>为什么两个表面上相同的 GPU 会有如此不同的内部配置？芯片之间的差异源于地板扫描，这是 NVIDIA 近年来越来越多地采用的技术。随着 GPU 芯片尺寸不断增大以满足不断增长的计算需求，芯片制造错误的可能性也随之增加。扫地技术可以通过熔断（“扫除”）有缺陷的部件来使用这些不完美的模具，从而使其余部件能够正常工作。</p>
<h4 id="分区特性"><a href="#分区特性" class="headerlink" title="分区特性"></a>分区特性</h4><p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025132903987.png" alt="image-20241025132903987"></p>
<ul>
<li>分区纵轴上来看是有洞的。</li>
<li>即使后续内核可能需要一组互斥的 SM，GPU 也会保留流中内核的顺序。的流2中，K9只需要SM 5，而K10只需要SM 10。但是，K10直到K9完成才开始。 CUDA 流语义要求同一流中的后续内核在所有先前的内核完成之前不会启动 - 我们发现即使在使用 TPC 分区时也要保留这一点。【既然要保持顺序你划分分区有什么用？是不同block的并行？】</li>
<li>适用于多个型号的gpu。</li>
</ul>
<h4 id="可能的硬件故障"><a href="#可能的硬件故障" class="headerlink" title="可能的硬件故障"></a>可能的硬件故障</h4><ul>
<li>WDU导致的SM非必要重叠：K2只能用1-3，但是被可以更加灵活分配的K1霸占了。</li>
</ul>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025134354958.png" alt="image-20241025134354958"></p>
<ul>
<li>任务槽被耗尽，优先级低的TMU被WDU一脚踢出去。</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>关于 GPU 计算单元空间分区的现有工作原理发现，计算单元到分区的分配对于 AMD GPU 至关重要，因为在某些情况下，向分区添加计算单元可能会减慢分区速度 。这项工作提出并评估了两种分区策略：SE 打包和 SE 分布式。 SE 代表着色器引擎，相当于我们上下文中的 GPC。 SE-packed 算法尝试将同一 GPC 中的所有 TPC 分配到该分区，然后再将分区扩展到多个 GPC。或者，SE 分布式算法尝试将每个分区的 TPC 尽可能均匀地分布在 GPC 上。由于这种语言已被其他工作采用，我们在这里继续使用它。这两种方式对性能差异不大，因此使用更加简单的SE打包。</p>
<h4 id="实验主体"><a href="#实验主体" class="headerlink" title="实验主体"></a>实验主体</h4><p>在yolov2上跑</p>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025134935385.png" alt="image-20241025134935385"></p>
<p>TPC 分区可以防止 GPU 上下文共享任务因计算密集型任务而导致性能下降。（参考上表234）</p>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025135011722.png" alt="image-20241025135011722"></p>
<ul>
<li>TPC 分区允许平滑调整任务执行时间，分配对时间有影响。</li>
<li>这种影响是有边际效用的。</li>
</ul>
<h2 id="知识学习"><a href="#知识学习" class="headerlink" title="知识学习"></a>知识学习</h2><h3 id="GPU架构"><a href="#GPU架构" class="headerlink" title="GPU架构"></a>GPU架构</h3><p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025130701643.png" alt="image-20241025130701643"></p>
<h4 id="GPU概述"><a href="#GPU概述" class="headerlink" title="GPU概述"></a>GPU概述</h4><p>GPU 是高度并行的加速器，通常由多个离散的功能单元构建，每个功能单元都具有内部并行能力。上图显示了此类最新 NVIDIA GPU 的内部功能单元。</p>
<ul>
<li>八个 GPC（通用处理集群），每个由 16 个 SM（流式多处理器）组成，共同构成 GPU 的计算&#x2F;图形引擎。</li>
<li>每个 TPC（线程处理集群）将 SM 以两个为一组排列。 每个 SM 包含 64 个CUDA 核心。</li>
<li>其他 GPU 引擎包括 5 个异步复制引擎、3 个视频编码引擎、1 个视频解码引擎和 1 个 JPEG 解码引擎。 先前的工作表明，这些引擎可以在一定程度上独立于计算&#x2F;图形引擎运行</li>
</ul>
<h4 id="CUDA概述"><a href="#CUDA概述" class="headerlink" title="CUDA概述"></a>CUDA概述</h4><p>为了简化这些复杂加速器（GPU）的编程，NVIDIA 开发了 CUDA 编程语言和 API。所有 CUDA 应用程序都在自己的内存地址空间（称为上下文）中运行，默认情况下，时分复用用于在 CUDA 和其他使用 GPU 的应用程序（例如显示任务）之间进行。</p>
<p>在单个cuda程序中，可以使用多个称为流的 FIFO 队列来允许工作完全并发地使用 GPU 。通常会将所有使用 CUDA 的功能组合到具有多个流的单个上下文中，以避免出现多个用不满gpu的小型应用程序被专门进行时间切片的情况 。</p>
<h4 id="任务术语【没看懂】"><a href="#任务术语【没看懂】" class="headerlink" title="任务术语【没看懂】"></a>任务术语【没看懂】</h4><p>任务（TASK)由作业(JOB)组成，其释放速率称为周期。发布后，每项工作都必须在随后的截止日期前完成。</p>
<ul>
<li><p>如果任务恰好在其周期开始时释放作业，则它们被称为周期性任务。</p>
</li>
<li><p>相反，如果该时间段仅定义版本之间的最小间隔，则该任务称为零星任务。任务可能具有相关的关键性。</p>
<p>这反映了它们在系统中的相对重要性。例如，在自动驾驶汽车中，行人检测任务的重要性较高，而屏幕显示更新的重要性较低。</p>
</li>
</ul>
<h3 id="TMD调度流程总结"><a href="#TMD调度流程总结" class="headerlink" title="TMD调度流程总结"></a>TMD调度流程总结</h3><p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025102939062.png" alt="image-20241025102939062"></p>
<h4 id="内核初始化"><a href="#内核初始化" class="headerlink" title="内核初始化"></a>内核初始化</h4><p>内核实例化。在 NVIDIA GPU 上，内核通过前面提到的 TMD 结构进行内部描述。它描述了内核所需的线程块、每个块的线程和共享内存资源，并包括内核入口点的地址（以及优先级等字段）。用户空间库（例如 CUDA 或 OpenCL）构造 TMD 后，会将包含指向 TMD 的指针的启动命令排队到命令段中的下一个可用命令槽中。命令段是包含 GPU 命令的连续内存块。 TMD 在整个调度管道中用作内核描述符和句柄（引用），一直持续到 GPU 内核的所有计算完成。</p>
<h4 id="②HostInterface"><a href="#②HostInterface" class="headerlink" title="②HostInterface"></a>②HostInterface</h4><p>主机接口。GPU 的主机接口弥合了 TMD 从 CPU 到 GPU 的路程。该单元包含一个或多个 Pushbuffer 直接内存访问单元 (PBDMA)、上下文切换控制逻辑以及其他子单元。 </p>
<ul>
<li>PBDMA 通过间接缓冲区从用户空间加载命令（GPU版RDMA？），间接缓冲区是指向命令段的指针的循环缓冲区。 这些结构（如图 6 顶部所示）允许用户空间应用程序直接将命令分派到 GPU，而无需系统呼叫或驱动程序开销，少了一层中介。PBDMA 单元单独加载、解析和缓存命令的速度比大多数 GPU 引擎快得多。</li>
<li>在图中，我们将 GPU 命令队列显示为位于 CPU 内存中（这是最常见的），但它们也可以位于 GPU 内存中。在命令获取和解析之后，主机接口将命令转发到适当的引擎。在内核启动的情况下，它将 TMD 传递到计算前端。</li>
</ul>
<h4 id="③Compute-FrontEnd"><a href="#③Compute-FrontEnd" class="headerlink" title="③Compute FrontEnd"></a>③Compute FrontEnd</h4><p>计算前端将 TMD 指针从主机接口 2 中继到任务管理单元 4。此外，我们了解到该单元可以在后续单元中协调上下文切换。 虽然该单元以与其他单元解耦的速率处理 TMD，但不正确的上下文context可能会导致工作在此排队堵塞。</p>
<h4 id="④TMU"><a href="#④TMU" class="headerlink" title="④TMU"></a>④TMU</h4><p>任务管理单元按优先级和到达顺序对TMD进行排队，直到工作分配单元WDU准备好接收它们。由于 TMU 的明确调度职责，我们对其进行了深入研究。</p>
<p>TMU 围绕一系列优先级单链表构建，每个优先级都有一个链表头和尾指针跟踪。每个列表都完全由TMD组成。</p>
<p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025125632852.png" alt="image-20241025125632852"></p>
<p>图7中，我们展示了一个支持三个优先级的单元，TMD 按到达序进行了说明性编号。三个 TMD 位于最高优先级列表中，没有一个 TMD 位于中优先级列表中，两个 TMD 位于最低优先级列表中。每个优先级列表正式称为一个 TMD 组。 这些列表允许 TMU 对其接收到的 TMD 进行重新排序，从而使优先级较高的 TMD 跳过优先级较低的 TMD。当 TMD 从计算前端或其他地方到达时， TMU 读取 TMD 的 GROUP_ID 字段，并将 TMD 附加到指定 TMD 组的尾部。例如，在图 7 中，TMD4 是最近到达的高优先级TMD，TMD2是最近到达的低优先级TMD。【优先级别重排】</p>
<p>当WDU发出另一个 TMD 准备就绪的信号时，TMU 删除并传递最高优先级非空列表的头部。这在图 7 中用粗体轮廓和“下一个 TMD”框来说明。给定图 7 的 TMU 状态，TMD 将按以下顺序退出 TMU：TMD1、TMD3、TMD4、TMD0【先到先得】，然后是 TMD2。 TMU 是可以以与 TMD 完成率解耦的速率接收 TMD 的最终单元。当用户空间以比其完成速度更快的速度分派内核时，这会导致 TMD 最终在 TMU 中累积。【队伍太长】</p>
<h4 id="⑤WDU"><a href="#⑤WDU" class="headerlink" title="⑤WDU"></a>⑤WDU</h4><p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025105532844.png" alt="image-20241025105532844"></p>
<p>WDU将可用的TMD移动到TPC上，任务槽的数量受到硬件限制。例如，当就绪内核的数量超过任务槽的数量时wdu也要决策。</p>
<p>每当任务槽变得可用时，WDU就向TMU发送新TMD的信号。然后，它将TMD插入到任务表中的任务槽中，并将对<strong>TMD的引用</strong>插入到按优先级排序的任务表中。优先级排序表首先按优先级排序，然后按到达时间排序。负载均衡器从优先级排序表头部的 TMD 调度线程块。一旦 TMD 的所有块都启动了，它就会从优先级排序表中删除，但会保留在任务表中，直到所有块都完成。【优先级表只提供开始顺序】。我们通过 TPC 资源跟踪器的阴影反映哪些内核正在哪些 TPC 上执行：这些单元在 TPC 和 WDU 之间中继状态和命令。请注意 TMD1 仍在 TPC 上执行（浅阴影线），因此保留在任务表中。 TMD3 作为目前最高优先级和最早到达的 TMD，现在正在向 TPC 调度块。 TMD4 正在等待 TMD3 调度其所有块，然后才能移动到优先级表的头部并调度块。</p>
<p>此排序和分派过程可能会受到两件事的干扰：TPC 分区和 4（TMU）中待处理的更高优先级工作。</p>
<ul>
<li>当使用TPC分区时，如果某个TPC有可用容量，但排序表头部的TMD被禁止在该TPC上执行，则WDU将在表中向前跳跃，直到找到允许在该TPC上执行的TMD。【你不允许在这个TPC上就找下一个】</li>
<li>当所有WDU任务槽都被占用，并且存在一个任务槽包含优先级低于TMU 中任何待处理的TMD时，较低优先级的TMD将从WDU中逐出并用较高优先级的TMD替换。当这种情况发生时，被驱逐的 TMD 会停止调度区块并完成已经在进行中的区块。然后将被逐出的TMD重新插入到TMU中相应优先级列表的头部。【有高优先级TMD来了就让位，重新排队】</li>
</ul>
<h3 id="内存并行性"><a href="#内存并行性" class="headerlink" title="内存并行性"></a>内存并行性</h3><p><img src="/./../images/Hardware-Compute-Partitioning-on-NVIDIA-GPUs-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241025110816069.png" alt="image-20241025110816069"></p>
<h4 id="⑥Execution-engine-and-memory-subsystem"><a href="#⑥Execution-engine-and-memory-subsystem" class="headerlink" title="⑥Execution engine and memory subsystem."></a>⑥Execution engine and memory subsystem.</h4><p>高速缓存的“I”（或“D”）后缀表示指令（或数据）高速缓存。</p>
<p>从图 的底部开始，内存分区单元与每个 DRAM 芯片为一对一关系，每个分区单元包含一个 DRAM 控制器和 L2 缓存的子集。每个分区单元通过交叉总线独立地连接到每个GPC，因此对内存分区单元进行分区也等价于对交叉总线和L2进行分区。</p>
<p>每个 GPC 内部的交叉总线与内存管理单元（MMU，以及相关的转换后备缓冲区，TLB）链接以提供虚拟内存支持。它连接到每个 SM 中的 L1D数据缓存，以及 GPC 范围的 L1.5I 缓存。 L1.5I 分别为每个 SM 指令高速缓存提供数据。</p>
<p>总之，理论上每个 GPC 都可以配置为使用 GPU 缓存、总线和 DRAM 资源的专有子集进行操作（如果内存分区单元已分区）。此外，每个 SM 都可以从其 L1 高速缓存进行操作，而不会产生干扰。</p>
<p>总结本节，我们发现 GPU 硬件不仅能够同时向多个GPC分区提供作业，而且还能够为分区提供无竞争的缓存、总线和 DRAM 资源。</p>
]]></content>
      <categories>
        <category>论文</category>
        <category>GPU架构</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>Inducer论文笔记</title>
    <url>/2024/12/18/Inducer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications论文笔记</title>
    <url>/2024/11/01/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>GPU 对于最大化深度神经网络 (DNN) 应用的每瓦吞吐量至关重要。然而，DNN 应用程序经常未充分利用 GPU，即使使用大批量大小并消除输入数据处理或通信停顿也是如此。 DNN 工作负载由依赖于数据的算子组成，具有不同的计算和内存要求。虽然算子可能会使 GPU 计算单元或内存带宽饱和，但它通常会使其他 GPU 资源闲置。尽管 GPU 共享技术很流行，但当前的方法还不够细粒度或干扰感知，无法最大限度地提高 GPU 利用率，同时最大限度地减少 10 μs 粒度的干扰。我们提出了 Orion，这是一个可以透明地<strong>拦截</strong>共享 GPU 的多个客户端的 GPU 内核启动的系统。 Orion 以各个<strong>算子的粒度</strong>安排 GPU 上的工作，并通过考虑每个算子的计算和内存需求来最大限度地减少干扰。我们将 Orion 集成到 PyTorch 中，并在各种 DNN 工作负载搭配用例中展示其优势。与高优先级推理作业的最先进基线相比，Orion 显着改善了<strong>尾部延迟</strong>，同时搭配尽力而为推理作业，将每个 GPU 请求吞吐量提高高达 7.3 倍，或者搭配 DNN 训练时，节省高达 1.49 倍× 与<strong>专用 GPU</strong> 分配相比，训练成本降低。</p>
<span id="more"></span>

<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>DNN在gpu上消耗更大，使用DNN作业的最终成本取决于硬件的高利用率，单个DNN无法充分利用GPU硬件：</p>
<ul>
<li>RT（延迟关键）任务由于批量较小导致并行性不足，无法保持GPU计算单元繁忙。</li>
<li>训练作业会最大化批量大小以提高吞吐量，但也要基于模型收敛的情况进行权衡，且训练作业可能因等待输入数据或通信瓶颈导致GPU空闲。</li>
</ul>
<p>不考虑输入数据和通信的延迟，DNN工作负载仍然无法充分利用GPU硬件：DNN 工作负载由许多运行较短时间的数据相关算子组成，每个算子都有不同的计算和内存要求。由于个别算子使<strong>计算单元</strong>或<strong>内存带宽</strong>饱和，但利用率通常会处于空闲状态，因此利用率呈突发性且平均较低。</p>
<p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106094015904.png" alt="image-20241106094015904"></p>
<p>常见的解决方案是在作业之间共享 GPU。主要挑战是最大限度地提高利用率，同时减少作业之间的干扰以获得高性能：</p>
<ul>
<li>时间共享技术以推理请求或训练小批量的粒度对 GPU 进行时间切片（时分复用）。这可能会导致阻塞情况，因为传入的推理请求或训练小批量需要等待正在进行的任务在 GPU 上完成执行，并且当单个任务的运算符执行时仍然会浪费资源。</li>
<li>空间共享提高了利用率，但是，当前的技术要么太粗粒度，要么没有足够的干扰感知（比如更好地分配BE和RT任务。减少碎片）的能力。</li>
</ul>
<p>为了帮助缩小这一差距，我们提出了 Orion，一种细粒度、干扰感知的 GPU 调度程序。 Orion 保持高优先级工作负载的性能，同时配置尽力而为的作业，以最大限度地提高 GPU 利用率并节省成本。 Orion是一个拦截GPU内核启动的软件系统，根据客户端作业优先级、算子大小以及算子是否受计算或内存限制来调度请求。通过按单个算子的粒度进行调度，Orion 在空间上共享 GPU，以充分利用 GPU 计算单元和内存带宽，而高优先级作业可能仅在 10-1000 μs 的时间内未充分利用这些带宽。</p>
<p>Orion 提高了各种 DNN 搭配用例的 GPU 资源效率（和成本），同时对高优先级作业性能的影响最小。</p>
<ul>
<li>当将延迟敏感推理与尽力而为离线推理相结合时，与专用 GPU 执行相比，Orion 将总吞吐量提高了 7.3 倍，同时将高优先级作业的 p99 延迟平均保持在 15% 以内。</li>
<li>当将延迟敏感的推理作业与训练作业搭配使用时，Orion 将 p99 （99%的推理延迟）推理延迟平均保持在 14% 以内，同时将 GPU 的总吞吐量提高至 2.3 倍。</li>
<li>Orion 在配置训练作业时将成本降低了 1.29 倍，同时确保高优先级训练作业的吞吐量保持在其专用 GPU 吞吐量的 <strong>16% 以内</strong>。</li>
</ul>
<h2 id="Orion架构"><a href="#Orion架构" class="headerlink" title="Orion架构"></a>Orion架构</h2><p>我们提出了 Orion，一种细粒度、干扰感知的 GPU 调度程序。 Orion 的目标是保持高优先级作业的高性能，同时将备用 GPU 资源用于尽力而为的作业来控制GPU算子提交。 Orion 对最终用户是透明的，不需要更改 API。我们将 Orion 实现为动态链接库。</p>
<p>如图 5 所示，Orion 拦截每个客户端提交的 GPU 算子，并将算子缓冲到每个客户端软件队列中。操作包括 GPU <strong>内核</strong>【<strong>一些深度学习的GPU操作</strong>】（例如，卷积、批量归一化）和内存管理操作（例如，内存分配、内存复制）。 Orion 使用第 5.1 节中描述的调度策略将操作从每个客户端软件队列提交到 GPU 硬件，并利用在离线工作负载分析阶段收集的内核特征（第 5.2 节中所述）。 Orion 在单个 GPU 设备的级别上运行。在分布式 DNN 作业部署中，每个 GPU 设备运行一个单独的 Orion 实例</p>
<p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106105310581.png" alt="image-20241106105310581"></p>
<h3 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h3><h4 id="kernel调度"><a href="#kernel调度" class="headerlink" title="kernel调度"></a>kernel调度</h4><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106105525146.png" alt="image-20241106105525146"></p>
<ol>
<li>首先拿出be和高优先级的，并且执行高优先级内核</li>
<li>如果有be任务，则考虑be任务能否调度（12行，be任务运行时间是否小于高优先级时间*（1-dur阈值），防止be任务影响高优先级任务调度）（27行，考虑be任务的SM数量会不会超过阈值，如果高优先级任务吞吐量大可以调高SM阈值），如果可以调度就将其加入调度队列，并更新duration时间。</li>
</ol>
<h4 id="调度方法论"><a href="#调度方法论" class="headerlink" title="调度方法论"></a>调度方法论</h4><ul>
<li>GPU流优先级：闭源GPU不会暴露抢占方法给你，但可以通过流优先级进行优先级调度，并通过DUR_THRETHOD最小化抢占带来的影响。</li>
<li>CUDA事件：CUDA Events 提供了一种监视 GPU 中每个流的进度的方法，而无需昂贵的流同步操作，这会阻塞 CPU 调度程序线程。提交尽力而为内核后，Orion 将其提交记录在 CUDA 事件中（第 21 行）。提交的内核完成后，GPU 将事件状态设置为“完成”。 Orion 使用 cudaEventQuery 查询尽力而为流的状态而不阻塞。</li>
</ul>
<h4 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h4><p>这段话描述了 Orion 系统在 GPU 资源管理上的方法，以及未来的改进方向，重点在于 GPU 核心 (Streaming Multiprocessors, SM) 调度和内存操作管理。以下是分解的主要内容：</p>
<ol>
<li><p><strong>GPU Kernel 调度策略</strong>：Orion 使用 §5.1.1 中定义的策略，将 GPU 内核（kernel）分配到不同的 SM 上。这意味着 Orion 会合理分配 GPU 计算资源，以提升任务执行效率。</p>
</li>
<li><p><strong>内存操作管理</strong>：</p>
<ul>
<li><strong>CPU-GPU 传输带宽的使用</strong>：内存分配和数据传输主要依赖于 CPU-GPU 间的 PCIe 带宽。当前设计中，Orion 直接将内存操作任务提交给 GPU。</li>
<li><strong>带宽干扰管理的未来计划</strong>：Orion 未来计划增加带宽干扰管理的功能，即在分配带宽时考虑带宽的使用率，以避免不同内存操作间的相互干扰。</li>
</ul>
</li>
<li><p><strong>内存操作的处理方式</strong>：</p>
<ul>
<li><strong>同步内存操作</strong>：对于同步操作（例如 <code>cudaMemcpy</code>、<code>cudaMemset</code>），Orion 会在 GPU 完成操作前暂停主机端的执行，以确保数据传输的完成。</li>
<li><strong>异步内存操作</strong>：对于异步操作（如 <code>cudaMemcpyAsync</code>），Orion 会在捕获这些操作后立即允许主机端继续执行，以提高效率。</li>
<li><strong>需要设备同步的操作</strong>：某些操作（例如 <code>cudaMalloc</code>、<code>cudaFree</code>）会导致设备同步。Orion 在这种情况下会同步所有客户端，确保不会出现无效的内存访问。</li>
</ul>
</li>
<li><p><strong>内存管理假设</strong>：当前 Orion 假设集群管理器会合理地将能够在 GPU 内存中放下的任务进行同置（collocate），类似于 REEF 中的假设。</p>
</li>
<li><p><strong>GPU 内存交换机制的兼容性</strong>：Orion 兼容现有的 GPU 内存交换机制（例如 NVIDIA 的 Unified Memory）。此外，它也可以结合更复杂的交换机制，如 Salus、PipeSwitch、ClockWork 和 vLLM 中提出的技术。</p>
</li>
<li><p><strong>未来优化方向</strong>：Orion 计划集成逐层卸载 (layer-by-layer offloading)，即保持高优先级任务留在 GPU 中，并将次要任务逐层载入&#x2F;卸出 GPU 内存。这种方法在高优先级任务所需内存不足时尤为有效。随着 GPU-CPU 互连带宽的提升，这种交换机制的开销预计会逐渐降低。</p>
</li>
</ol>
<p>总结来说，Orion 的设计关注在提高 GPU 资源利用率和内存管理效率，同时也在持续改进以支持更加复杂的内存交换机制。</p>
<h3 id="工作负载分析"><a href="#工作负载分析" class="headerlink" title="工作负载分析"></a>工作负载分析</h3><p>Orion 的调度策略需要有关每个内核的计算与内存强度、每个尽力而为作业内核的预期执行时间和 SM 要求以及高优先级作业的请求延迟的信息。在执行之前，Orion 会离线分析每个 DNN 工作负载，并生成一个包含模型中每个内核的配置信息的文件。 Orion 调度程序将分析信息加载到内存查找表中，并按唯一的内核 ID 进行索引。</p>
<p><strong>内核延迟和资源配置文件。</strong> Orion 使用 NVIDIA 的 Night Compute [26] 和 Nsight Systems [27] 工具来收集每个内核的计算吞吐量、内存吞吐量和执行时间。我们使用 Nsight Compute 中的屋顶线分析，将内核分类为计算限制型或内存限制型。由于该工具不包括所有内核的屋顶线分析，因此如果计算吞吐量或内存带宽利用率分别超过 Nsight 计算工具的建议的 60%，我们将进一步将内核分类为计算或内存限制。如果计算吞吐量和内存带宽利用率均低于 60%，并且内核无法进行屋顶线分析，我们将其资源配置文件分类为未知。在实践中，未知文件通常执行时间很短，影响不大。</p>
<p><strong>内核 SM 要求。</strong>对于 besteffort 作业中的每个内核，Orion 使用 Nsight 计算工具获取块数、块的线程数、每个线程的寄存器数以及内核所需的共享内存量。对于每个内核 k，我们首先确定blocks_per_smk，它是该内核的目标 GPU 架构上每个 SM 可支持的块数。 block_per_smk 可能受到线程数量、寄存器数量或内核 k 所需的每个 SM 可用共享内存量的限制。然后，我们计算每个内核所需的 SM 数量： sm_neededk &#x3D; ceil(num_blocksk &#x2F;blocks_per_smk )。</p>
<p><strong>请求延迟。</strong>为了确定 DUR_THRESHOLD 参数（Orion 使用该参数根据相对于高优先级请求执行的持续时间来限制尽力而为内核启动），当作业在专用 GPU 中单独运行时，Orion 还必须分析高优先级请求延迟。对于推理作业，一个请求是指单批推理请求。对于训练作业，请求指的是单个训练迭代。</p>
<h3 id="与DNN框架整合"><a href="#与DNN框架整合" class="headerlink" title="与DNN框架整合"></a>与DNN框架整合</h3><p>Orion 的调度策略与应用程序框架无关。 Orion 动态链接到 DNN 框架，并且对最终用户透明。 </p>
<p>PyTorch 原型。我们在 PyTorch [79] 中用 3000 行 C++&#x2F;CUDA 代码实现了 Orion。在本机 PyTorch 中，客户端应用程序使用 CUDA 运行时 API [14] 以及 CUBLAS [19] 和 CUDNN [20] 等库启动内核，这些库为常见的 DNN 操作提供高性能实现。 Orion 通过使用包装函数覆盖它们来拦截 CUDA 内核启动，这些函数将必要的信息（内核标识符和参数）提交到每个客户端软件队列。 Orion 目前实现了来自 CUDA 运行时 API 的内存管理操作（cudaMalloc、cudaMemcpy、cudaMemset、cudaFree 等）和内核启动操作（cudaLaunchKernel）的包装器，以及用于卷积、批量归一化和矩阵矩阵乘法的 CUDNN 和 CUBLAS 函数。这些包装器足以支持我们评估中的所有 DNN 工作负载，但还可以添加更多。拦截操作和管理每个客户端软件队列是轻量级的，使用 Orion 包装器的开销不到 1%。</p>
<p>在我们当前的原型和评估中，客户端应用程序和 Orion 调度程序作为同一进程的不同线程运行，从而实现进程内内存共享和快速通信。 Orion 还可用于作为不同进程执行的应用程序。在这种情况下，Orion 作为单独的进程执行，客户端将内核提交到共享内存区域中的队列。这就需要GPU支持多个进程的并发访问，比如NVIDIA的MPS功能。  </p>
<h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>我们评估Orion 是为了回答以下关键问题： </p>
<ul>
<li>Orion 的性能与其他GPU 共享方法相比如何？ </li>
<li>与为每项作业使用专用GPU 相比，使用Orion 在成本和GPU 利用率方面有何优势？</li>
<li>Orion 调度策略的哪些方面对性能优势贡献最大？</li>
<li>Orion 如何推广到新的 GPU 架构？</li>
<li>Orion 如何扩展到多个尽力而为的客户？</li>
<li>Orion 的内核分析和内核启动拦截机制的开销是多少？</li>
</ul>
<h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h3><p><strong>实验试验台：</strong>我们使用 Google Cloud n1-standard-8 VM 在 NVIDIA V100-16GB GPU 上评估 Orion，该 VM 具有 8 个 vCPU 内核和 30 GB DRAM。我们将 PyTorch 1.12 与 Python 3.9 和 CUDA 10.2 结合使用。我们还通过使用 a2-highgpu-1g VM 和 CUDA 11.3 在 A100-40GB GPU 上评估 Orion，展示了 Orion 可以推广到其他 GPU 架构。</p>
<p>对于所有实验，我们确保作业执行时没有数据预处理或通信瓶颈。因此，我们评估了 Orion 提高 GPU 利用率的能力，同时最大限度地减少最具挑战性的环境中的干扰，其中每个单独的作业都最大化其自己的 GPU 利用率。我们将每个实验重复三次。</p>
<p><strong>工作负载：</strong>我们考虑三种常见的 GPU 共享用例。首先，我们将高优先级、延迟敏感的推理作业与尽力而为的训练作业（inf-train）搭配起来。接下来，我们搭配高优先级和尽力而为的训练（train-train）。最后，我们将高优先级、延迟敏感的推理作业与尽力而为的离线推理作业（inf-inf）结合起来。对于每个用例，我们都会考虑计算机视觉和自然语言处理 (NLP) 领域的流行 DNN 模型。 ResNet50、ResNet101 [51] 和 MobileNet-v2 [84] 是代表性的视觉模型。我们使用他们的 TorchVision 实现 [28]。 BERT [43] 和 Transformer [92] 是代表性的 NLP 模型。我们使用 NVIDIA [22] 的实现。我们使用全精度进行训练和推理。</p>
<p>表 1 总结了每个工作负载的批量大小。我们将批量大小与之前在相同或类似 GPU 平台上的工作中使用的批量大小进行匹配 [21,23,51,54,84,97]。我们考虑推理作业的均匀分布和泊松请求到达分布。均匀分布代表自动驾驶等应用领域（例如，摄像机检测障碍物 [36, 37]），而泊松到达代表事件驱动的实时 DNN 应用（例如语音识别 [50, 52]） ]）。平均请求到达率（如表 3 所示）与 Microsoft Azure Functions 跟踪 [87] 中前 20 个最常执行的函数的平均调用请求率相匹配，如其他作品 [49, 105] 中用于压力测试的那样GPU搭配场景。对于视觉模型，我们还使用从阿波罗自动驾驶系统中部署的真实物体检测模型收集的推理轨迹[36]。该推理跟踪来自 DISB 推理服务基准 [17]，首先用于评估 REEF [50]。对于 Apollo 跟踪的实验，我们将跟踪的调用时间戳用于高优先级推理作业，并假设并置的尽力而为推理作业的请求到达时间一致。同时，训练作业在闭环中提交请求。</p>
<p><strong>基线。</strong>我们将 Orion 与时间共享、NVIDIA MPS [25] 和 GPU Streams [3] 空间共享机制进行比较。 GPU 流允许多个客户端应用程序共享 GPU，只要它们属于同一进程即可。因此，对于此基线，我们将每个 DNN 应用程序客户端作为单独的线程运行，该线程将请求提交到单独的 CUDA 流。我们为高优先级作业分配一个高优先级流，为每个尽力而为的作业分配一个默认优先级流。 MPS 是计算能力 3.5 或更高的 NVIDIA GPU 中的一项功能，它允许多个进程在空间上共享 GPU。</p>
<p>我们还与最先进的 REE-N进行比较，该规则根据内核的大小（SM 数量）和预期延迟来调度内核。根据与 REEF 作者的讨论，我们使用 12 个内核的软件队列大小。虽然 REEF 主要设计用于并置推理作业（因为训练作业包括更新模型状态的非幂等内核），但 REEF-N 可以安全地用于训练作业，因为内核执行永远不会被抢占。因此，我们将 Orion 与 REEF-N 策略的所有搭配用例进行比较。（没有抢占如何凸显reef的有点？）</p>
<p>对于训练作业搭配（训练-训练），我们将 Orion 与 Tick-Tock [94] 进行比较，后者抵消了训练小批量迭代的前向和后向传递，以最大限度地减少聚合内存使用并减少干扰。 Zico [67]也实现了这种方法。由于这两个系统都没有可用的 PyTorch 开源实现，因此我们根据论文实现了该方法。作为性能上限，我们测量专用 GPU 上每个工作负载的延迟和吞吐量。理想基线的延迟等于高优先级作业的延迟，没有搭配，吞吐量等于高优先级作业和尽力而为作业的专用 GPU 吞吐量之和。该基线是延迟的下限和吞吐量的上限。</p>
<h3 id="实验主体"><a href="#实验主体" class="headerlink" title="实验主体"></a>实验主体</h3><h4 id="推理-训练"><a href="#推理-训练" class="headerlink" title="推理-训练"></a>推理-训练</h4><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106111902249.png" alt="image-20241106111902249"></p>
<p>问题：这吞吐量哪里好了？</p>
<p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106134538057.png" alt="image-20241106134538057"></p>
<p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106134607263.png" alt="image-20241106134607263"></p>
<h4 id="训练-训练"><a href="#训练-训练" class="headerlink" title="训练-训练"></a>训练-训练</h4><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106140615287.png" alt="image-20241106140615287"></p>
<p>Orion训练的时间成本花费也是最少的。</p>
<h4 id="训练-推理"><a href="#训练-推理" class="headerlink" title="训练-推理"></a>训练-推理</h4><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106141246063.png" alt="image-20241106141246063">好像没有吞吐量的图？但其实吞吐量结果也不错。</p>
<h4 id="更多的GPU和客户端"><a href="#更多的GPU和客户端" class="headerlink" title="更多的GPU和客户端"></a>更多的GPU和客户端</h4><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106141420118.png" alt="image-20241106141420118"></p>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106141446617.png" alt="image-20241106141446617"></p>
<p>我们首先将每个客户端分配给不同的 CUDA 流，每个流都具有默认优先级。图 14 中的 GPU 流栏显示此方法具有较高的延迟。流优先级基线显示，对高优先级推理作业的流使用最高 CUDA 优先级有助于将 p95 延迟减少高达 25%。添加 Orion 策略的第一个组件（根据计算内存资源配置文件调度 besteffort 作业内核）可将 p95 延迟额外减少 48%。最后，考虑到内核的大小（SM 数量），除了计算&#x2F;内存之外，延迟还可减少高达 54%。因此，计算&#x2F;内存感知和大小感知调度大致同等重要。</p>
<p>然后，在应用计算&#x2F;内存配置文件和基于内核大小的调度后，我们检查流优先级对于 Orion 系统是否至关重要。流优先级机制目前仅具有微小的改进，因此 Orion 也可以用于 GPU 硬件不支持流优先级的设置（例如，在 MPS 模式下 [46]）。我们还调整了 DUR_THRESHOLD。我们发现 Orion 在 DUR_THRESHOLD 值低于 3% 时具有稳定的性能。由于尽力而为内核的限制较少，DUR_THRESHOLD 的线性增加超过 3% 会导致高优先级作业性能近似线性下降。例如，将 ResNet101 推理与尽力训练结合使用时，对于 DUR_THRESHOLD 值为 10%、15% 和 20%，推理延迟分别为 23ms、26ms 和 30ms，而尽力训练吞吐量为 8.7、9.26 和9.75 次迭代&#x2F;秒。用户可以根据高优先级作业服务级别目标调整 DUR_THRESHOLD。我们在实验中使用 2.5%。</p>
<h4 id="开销"><a href="#开销" class="headerlink" title="开销"></a>开销</h4><p>内核启动拦截。我们使用 Orion 的内核拦截机制直接调度内核来测量专用 GPU 上每个推理和训练作业的执行时间。与原生 PyTorch 相比，Orion 在所有作业中的开销仍然低于 1%。内核资源分析。我们使用 NVIDIA 的 Nsight Systems (NSYS) 和 Nsight Compute (NCU) 工具来分析训练作业的前 10 个小批量或推理作业的 10 个请求。 Nsys 工具在迭代时间中增加了高达 5% 的开销。 NCU 工具对每个内核执行更详细的资源分析（例如缓存未命中、warp 调度程序统计信息），并且分析时间与内核数量成正比。在我们的实验中，每个内核需要 ∼2-5 秒。由于分析是离线的，因此这些工具不会影响实际的作业执行。</p>
<h2 id="改进方向"><a href="#改进方向" class="headerlink" title="改进方向"></a>改进方向</h2><p><strong>集群管理器协同设计。</strong> Orion 目前是作为每 GPU 调度程序实现的。未来，我们计划探索集群管理的协同设计。通过使用每个作业的计算和内存强度内核配置文件，集群管理器可以将具有互补资源配置文件的作业放置在同一 GPU 上，以最大限度地提高资源利用率并减少干扰。</p>
<p><strong>软件&#x2F;硬件协同设计。</strong>由于 GPU 硬件目前向主机软件公开的接口有限，优化 GPU 利用率和性能具有挑战性。我们从 OpenSSD [59] 平台中汲取灵感，该平台通过向软件公开较低级别的接口来支持闪存存储设备控制器硬件和主机软件协同设计的研究。与软件&#x2F;硬件协同设计如何最大限度地减少对共享 SSD 的干扰 类似，启用 GPU 调度的软件&#x2F;硬件协同设计（例如，控制 SM 上 GPU 内核的放置）可以允许应用程序调整共享 GPU 上的端到端性能。随着 GPU 编程的最新趋势越来越多地将越来越多的调度任务转移到 GPU 硬件，这一点尤其重要。例如，CUDA 图 [6] 通过单个 CUDA API 调用来调度 GPU 中的整个内核图，以减少 CPU 启动开销。在这种情况下，Orion 的调度策略可以在 GPU 驱动程序或 GPU 调度程序级别实现，以交错来自多个图的内核，同时最大限度地减少干扰。</p>
<p><strong>GPU 缓存干扰。</strong>我们目前不考虑 GPU 缓存干扰。 NVIDIA 工具提供缓存未命中统计数据 [26]，可用于推断更具体的内核配置文件并更准确地模拟干扰。</p>
<p><strong>安全。</strong>我们假设共享 GPU 的客户端位于同一信任域中，这在同一组织运营的 DNN 集群中是合理的假设 [98, 99]。因此，Orion 最大限度地减少了性能干扰，但不能保证共享 GPU 的不受信任客户端之间的安全隔离。异构硬件的可信执行环境是一个活跃的研究领域[93]。</p>
<p><strong>对大型语言模型 (LLM) 的适用性</strong>。我们计划进一步研究 Orion 对大型语言模型的适用性 [38, 90]。之前的工作 [55, 60] 表明，LLM 推理的令牌生成阶段（一个令牌接一个令牌）顺序发生，是受内存限制的，同时未充分利用 GPU 的计算吞吐量和 SM。因此，我们可以采用 Orion 的资源感知调度策略将 LLM 推理与计算密集型工作负载并置。然而，LLM [38] 的大尺寸以及用于加速令牌生成的键值缓存 [81] 显着增加了 LLM 推理的内存需求。因此，当与其他工作负载并置时，必须采用额外的内存交换机制。如第 5.1.3 节所述，我们计划通过现有的 DNN 交换机制来增强 Orion。其中一种机制是 PagedAttention [60]，它为 LLM 推理提供动态分配和交换，并且可以与 Orion 无缝集成</p>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="GPU架构"><a href="#GPU架构" class="headerlink" title="GPU架构"></a>GPU架构</h3><p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106094035422.png" alt="image-20241106094035422"></p>
<h4 id="GPU编程抽象"><a href="#GPU编程抽象" class="headerlink" title="GPU编程抽象"></a>GPU编程抽象</h4><p>开发人员将他们的 DNN 应用程序定义为在 PyTorch或 TensorFlow框架中使用高级 API 的操作集合。应用程序框架为目标 GPU 架构编译这些操作（例如，卷积、批量归一化），并将操作作为 CUDA 计算内核提交给 GPU，和分配、初始化和释放 GPU 内存的 CUDA 内存管理操作行为类似。提交内核涉的流程及指定其资源需求（例如，线程块，寄存器、每块线程的数量以及所需的共享内存）。应用程序将每个内核启动和内存操作与特定的 CUDA 流相关联。流是保证按顺序执行的一系列操作。每个应用程序进程都有自己的默认流。为了增加并发性，应用程序可以创建额外的流（可以选择具有不同的优先级[15]）并跨流提交内核</p>
<h4 id="GPU硬件调度。"><a href="#GPU硬件调度。" class="headerlink" title="GPU硬件调度。"></a>GPU硬件调度。</h4><p>GPU 将每个 CUDA 流的内核缓冲在设备上的单独工作队列中。大多数 GPU不允许用户在提交后抢占内核 。 GPU 硬件调度程序根据<strong>流优先级</strong>从每个工作队列中的内核调度线程块。当满足线程块的数据依赖性并且具有足够资源的SM可用时，调度程序将线程块分配给SM。</p>
<p>尽管研究人员针对流行的 GPU 架构进行了逆向工程硬件调度策略，但用户无法控制哪个 SM 将执行特定的线程块。</p>
<p>当线程块被分配给 SM 时，SM 将调度并执行该块中的所有线程wrap。 SM 可以从属于不同内核和流的线程块同时执行多个 warp。</p>
<p>然而，如果任何 warp 使 SM 上的资源（例如寄存器数量）饱和，则 SM 的 warp 调度程序将等到没有资源饱和后再调度任何其他 warp，即使 SM 上的还有其他可用资源（例如计算单元或共享内存）。</p>
<h4 id="GPU利用率"><a href="#GPU利用率" class="headerlink" title="GPU利用率"></a>GPU利用率</h4><p>GPU 利用率指标。最常见的 GPU 利用率指标是 SM 利用率，即执行至少一个wrap的SM的比例。 SM 利用率并不能完全捕获 GPU 利用率，因为在执行wrap时即使只使用一小部分资源，SM 也会被视为繁忙。</p>
<p>计算吞吐量利用率是SM计算单元的利用率，例如FP32、FP64、FP16、FMA单元、张量核等。使用 NVIDIA Nsight 计算工具 ，我们可以获得有关每个单独组件的利用率的信息。报告的利用率是所有不同组件利用率的最大值。内存容量利用率是 GPU 上分配的内存的百分比。内存带宽利用率是消耗的峰值 GPU 内部内存带宽所占的百分比。</p>
<h3 id="DNN-GPU-利用率"><a href="#DNN-GPU-利用率" class="headerlink" title="DNN-GPU 利用率"></a>DNN-GPU 利用率</h3><p>尽管 DNN 应用程序通常具有较高的计算和内存强度，但它们经常未充分利用 GPU 资。之前的工作已经确定了 GPU 利用率低的原因并提出了解决方案。主机 CPU 上的输入数据预处理瓶颈可能会使 GPU 在等待摄取数据时处于空闲状态 。我们可以通过分解和扩展数据预处理来缓解输入停顿。节点之间的通信会限制分布式训练吞吐量和空闲 GPU 。积极的流水线技术、梯度压缩、异步更新、网络内聚合有助于隐藏通信停顿。多 GPU 集群中的组调度可能会使一些 GPU 空闲，而其他 GPU 可用。最近的 DNN 系统通过弹性 GPU 分配解决了这个问题 。</p>
<p>但即使不考虑数据通信和预处理，GPU利用率仍然存在瓶颈。面向大吞吐量作业，需要使用批量较大的数据训练，但是较大批量也会降低训练收益。而且各个技术是特定于模型的，需要大量的专业知识和特化调整。 </p>
<p><img src="/./../images/Orion-Interference-aware-Fine-grained-GPU-Sharing-for-ML-Applications%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241106102811657.png" alt="image-20241106102811657"></p>
<p>最近，大型语言模型 (LLM) 因其在各种任务中的高性能而变得非常流行。由于 LLM 具有异常大的内存容量要求，因此在 LLM 工作负载之间共享 GPU 的机会更加有限。因此，LLM 不是我们 GPU 共享的目标工作负载。尽管如此，在第 7 节中，我们讨论了 LLM 的 GPU 共享机会，因为 LLM 推理的顺序令牌生成阶段受内存限制，并且未充分利用 GPU 的计算吞吐量和 SM。</p>
<h4 id="分析-DNN-作业的-GPU-利用率"><a href="#分析-DNN-作业的-GPU-利用率" class="headerlink" title="分析 DNN 作业的 GPU 利用率"></a>分析 DNN 作业的 GPU 利用率</h4><p>GPU 计算吞吐量和内存带宽利用率是突发性的，并且平均较低。 GPU 计算利用率峰值通常发生在与内存利用率峰值不同的时间点。内核通常执行 10 到 100 μs（用于推理）或 100 到1000 μs（用于训练）。由于单个 DNN 作业的内核由于数据依赖性而按顺序执行，因此当内核使 GPU 计算或内存带宽饱和时，它通常会导致其他 GPU 资源短时间空闲。</p>
<h4 id="GPU内核搭配探索"><a href="#GPU内核搭配探索" class="headerlink" title="GPU内核搭配探索"></a>GPU内核搭配探索</h4><p>小实验证明，将C重内核和M重内核搭配可以提高GPU空间利用率。</p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul>
<li><strong>时间共享。</strong>时间共享技术通过多个作业之间的上下文切换对 GPU 进行时间切片，以提高利用率。先前的系统专注于在每个 GPU 上复用多个 DNN 模型，这些模型的集体状态不适合 GPU 内存。因此，这些系统解决的主要挑战是在特定模型的请求到达时有效地交换状态。 Gandiva [102] 使用挂起并重新启动机制在上下文切换期间在主机和 GPU 内存之间传输状态。 Salus [104] 通过优化 GPU 上应保留的状态来减少上下文切换。 Clockwork [49] 通过根据加载&#x2F;卸载 DNN 状态和运行推理的预期时间预先确定 GPU 是否能够满足请求截止日期，为每个 GPU 提供数千个具有可预测延迟的 DNN。 Antman [103] 动态调整作业内存分配，以实现每个 GPU 更高效的集群级作业配置，以实现时间共享。透明 GPU 共享 (TGS) [100] 为容器化工作负载提供与应用程序无关的临时 GPU 共享<strong>。然而，这些系统仍然一次执行一项作业。正如第 3 节中所讨论的，这未充分利用 GPU，因为单个 DNN 作业的内核通常不会消耗所有 GPU 计算单元和内存。</strong>我们细粒度、干扰感知共享的目标是为此类作业填补空闲的 GPU 容量。我们的工作补充了上述方法，有效地交换状态以适应每个 GPU 的更多模型。</li>
<li><strong>空间共享。</strong>空间共享机制使作业能够同时使用 GPU 的不同区域[106]。 NVIDIA 多实例 GPU (MIG) [12] 提供粗粒度 GPU 分区，但缺乏机会性地收集短时间段内未充分利用的资源的灵活性。创建新分区后，MIG 分区需要花费 100 毫秒的时间才能创建，模型需要 10 秒的时间才能从检查点恢复执行 [65]。 NVIDIA 多进程服务 (MPS) [25] 允许多个进程在 GPU 上并行运行，但会导致高干扰，因为进程可以自由共享缓存、计算和内存资源（见图 2）。 REEF [50] 根据内核的大小和优先级以细粒度调度内核，旨在并置高优先级和低优先级的推理作业。 Zico [67] 和 Tick-Tock [94] 通过调度前向和后向传递来在 GPU 上配置训练作业，以最大限度地减少总内存消耗。然而，<strong>这些方法都没有根据其计算和内存配置文件来共同调度内核，这对于最大限度地减少干扰同时最大化 GPU 利用率至关重要</strong>，我们在第 3 节中展示了这一点。</li>
</ul>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences论文笔记</title>
    <url>/2024/11/01/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>自动驾驶和虚拟现实等许多智能应用程序都需要运行延迟关键型和尽力而为的 DNN 推理任务，以在 GPU 上实现实时性和节省工作量。然而，商用 GPU 缺乏高效的抢占式调度支持，最先进的方法要么必须独占 GPU，要么让实时任务等待尽力而为的任务完成，这会导致利用率低或延迟高，或两者兼而有之。本文介绍了 REEF，这是第一个 GPU 加速的 DNN 推理服务系统，可在 GPU 调度中实现微秒级内核抢占和受控并发执行。 REEF 有两个新颖之处。首先，<strong>基于 DNN 推理内核大多具有幂等性的观察，REEF 设计了一种基于重置的抢占方案，通过主动杀死和恢复微秒级的尽力而为内核，在 GPU 上启动实时内核</strong>。其次，<strong>由于 DNN 推理内核具有不同的并行性和可预测的延迟，REEF 提出了一种动态内核填充机制，可以使用适当的尽力而为内核动态填充实时内核，从而以可忽略的开销充分利用 GPU</strong>。使用新的 DNN 推理服务基准 (DISB) 以及 AMD GPU 上的不同工作负载和真实世界跟踪进行的评估表明，REEF 在实时任务的端到端延迟方面仅产生不到 2% 的开销，但增加了与将 GPU 专用于实时任务相比，整体吞吐量提高了高达 7.7 倍。为了证明我们的方法在闭源 GPU 上的可行性，我们在 NVIDIA GPU 上进一步移植和评估了 REEF 的受限版本，将抢占延迟减少了 12.3 倍（从 6.3 倍）。</p>
<p>代码：https: &#x2F;&#x2F;github.com&#x2F;SJTU-IPADS&#x2F;reef</p>
<span id="more"></span>

<p>[TOC]</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="任务分类"><a href="#任务分类" class="headerlink" title="任务分类"></a>任务分类</h3><p>自动驾驶方面任务分为两种：</p>
<ul>
<li><p>对延迟敏感的实时任务，下文称RT（Real Time）。例如，自动驾驶汽车使用 DNN 来识别障碍物和交通信号灯</p>
</li>
<li><p>没有硬实时要求的任务[78]（在本文中称为尽力而为任务），例如监控人类驾驶员的情绪和疲劳，也在 GPU 中使用 DNN [19,48,84] 提供服务。</p>
</li>
</ul>
<p>通常，DNN 推理对于 GPU 调度有两个潜在冲突的目标。首先，实时任务应该被视为GPU上的一等公民，不受其他任务的干扰，以实现低端到端延迟。其次，实时任务和尽力而为任务应该在 GPU 上同时执行，以实现高整体吞吐量（节省工作量）。</p>
<h3 id="传统的GPU并行技术"><a href="#传统的GPU并行技术" class="headerlink" title="传统的GPU并行技术"></a>传统的GPU并行技术</h3><ul>
<li>前人提出了一种基于等待的方法来被动等待直到运行块完成，这可能会导致几毫秒的抢占延迟。但对RT任务来说影响较大，且当RT请求以高频率到达时，尽力而为的任务甚至可能会陷入饥饿，</li>
<li>最先进的 GPU 库（例如 CUDA [52] 和 ROCm [3]）通常提供多个 GPU 流（例如 CUDA Streams [60]）以在同一 GPU 上同时执行多个任务。然而，如图1（b）所示，尽管在独占GPU时RT任务的端到端推理延迟较低（约4ms）且稳定，但当与BE任务同时运行时，速度会提高一个数量级以上（接近 50 毫秒）。不幸的是，这对于实时场景来说是不可接受的。</li>
</ul>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101105618450.png" alt="image-20241101105618450"></p>
<p>与操作系统使用抢占式调度来提供实时保证类似，一种直观的方法是为 GPU 调度提供抢占式，遗憾的是商用 GPU 中缺少这种功能。本文介绍了 REEF，这是第一个用于商用 GPU 的 DNN 推理服务系统，具有微秒级内核抢占和 GPU 调度中的受控并发执行，以实现实时性和工作节省。具体来说，到达的实时任务应立即从正在运行的尽力而为内核中抢占 GPU，而无需等待其完成。同时，应使用实时内核剩余的 GPU 资源来并发执行尽力而为内核。</p>
<h3 id="REEF的主要思想"><a href="#REEF的主要思想" class="headerlink" title="REEF的主要思想"></a>REEF的主要思想</h3><ul>
<li>REEF 的一个关键见解是 DNN 推理中的每个内核大多是幂等的。这意味着正在运行的尽力而为内核可以主动终止并恢复，而无需保存上下文。（重复执行是没有副作用的）<strong>基于此，REEF提出了基于复位的抢占方案。为了彻底刷新 GPU 运行时和设备中的数百个未完成的内核，REEF 设计了不同的方法来重置不同的软件队列，并对 GPU 驱动程序进行改造，以准确使用现有的硬件机制来重置计算单元</strong>。因此，无论被抢占的内核数量及其执行时间如何，REEF 都可以在数十微秒内在 GPU 上启动实时任务。 </li>
<li>REEF 基于 DNN 推理中 GPU 内核的执行时间是确定性和可预测的观察结果，进一步提出了<strong>动态内核填充机制</strong>。这意味着可以根据提前的离线分析，仔细选择待处理的BE任务来填充实时内核，而不会干扰性能。<ul>
<li>REEF 扩展了 GPU 编译器，通过使用函数指针构造填充内核的模板。此外，为了消除 GPU 上间接函数调用的开销，REEF 引入了代理内核来解决寄存器分配问题并避免运行时不必要的上下文保存。因此，REEF 可以同时执行实时任务和尽力而为任务，而代价是可以忽略不计的性能和内存开销（小于 1%，大约 10 KB）。</li>
</ul>
</li>
</ul>
<p>我们通过扩展 Apache TVM [73]（深度学习编译器）和 AMD ROCm [3]（开源 GPU 计算平台）来实现 REEF。我们使用具有不同工作负载和模型的新 DNN 推理服务基准 (DISB) 以及来自 Apollo [7]（开放式自动驾驶平台）的真实跟踪来评估 REEF。我们的实验结果表明，与将 GPU 专用于实时任务相比，REEF 只产生实时任务不到 2% 的端到端延迟开销，但总体吞吐量提高了 4.3 倍。与最先进的技术相比，我们的方法进一步将抢占延迟减少了一个数量级以上，所有模型的抢占延迟均小于 40 微秒。为了证明我们的方法在闭源 GPU 上的可行性，我们在 NVIDIA GPU 上进一步移植和评估了 REEF 的受限版本，将抢占延迟减少了 12.3 倍（从 6.3 倍）。</p>
<h3 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h3><ul>
<li>深入了解 GPU 加速的 DNN 推理的特性（例如幂等性）和最先进的 GPU 调度方案的问题。</li>
<li>一种新的基于重置的抢占方案，无论被抢占的内核数量有多少，都可以在几微秒内启动 GPU 上的实时内核</li>
<li>一种优雅的机制，可以用尽力而为内核动态填充实时内核，以充分利用GPU 的大规模并行性</li>
<li>在 AMD 和 NVIDIA GPU 上的实施以及展示 REEF 相对于最先进技术的优势和功效的评估。</li>
</ul>
<h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="DNN特性"><a href="#DNN特性" class="headerlink" title="DNN特性"></a>DNN特性</h3><p>深度神经网络 (DNN) 包含多个多功能层实例，例如卷积层、池化层和全连接层。为了满足 GPU 上的推理请求，预先训练的 DNN 模型（例如 ResNet [30]）会提前加载到 GPU 内存中。图 2 概述了 GPU 加速的 DNN 推理的实现。对于每个到达的请求，DNN 的所有内核模型根据输入依次执行，结果输出返回到 DNN 应用程序。 DNN 推理现在被用于实时 (RT) 任务，例如障碍物和交通灯识别，以及尽力而为 (BE) 任务，例如情绪和疲劳监测。实时任务对延迟至关重要，因为违反端到端延迟要求可能会导致系统故障甚至安全问题。此外，此类请求通常由输入传感器以各种频率定期发出。相反，尽力而为任务没有硬性时序要求，而是在后台重复执行。</p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101111147938.png" alt="image-20241101111147938"></p>
<h4 id="幂等性"><a href="#幂等性" class="headerlink" title="幂等性"></a>幂等性</h4><p>用于推理任务的 GPU 加速 DNN 模型由一系列内核组成，这些内核乎仅由密集的线性代数计算组成，没有副作用。因此，<strong>无论是否重试，内核始终可以使用相同的输入产生相同的输出</strong>。同时，在 DNN 模型中，第 (k) 个内核始终使用第 (k-1) 个内核的输出和静态参数（例如权重）作为输入，例如图 2 中的 conv_relu 和稠密内核。因此，<strong>DNN推理任务的执行可以从被中断的内核之前的任何内核恢复，并且不会改变推理结果</strong>。</p>
<p>处理的数据也要回滚吧?不知道放在哪里.</p>
<h4 id="大量内核"><a href="#大量内核" class="headerlink" title="大量内核"></a>大量内核</h4><p>现代 DNN 模型中常见数百个内核。因此，大量的内核（通常是数百个或更多）将被提前提交来隐藏冗长的内核启动时间。此外，为了充分利用 GPU，服务系统可以使用相同或不同的 DNN 模型同时执行来自不同推理任务的多个内核。因此，抢占 GPU 的性能损失将是巨大的（几毫秒），甚至可以与数百个内核的执行时间相媲美。</p>
<h4 id="可预测性"><a href="#可预测性" class="headerlink" title="可预测性"></a>可预测性</h4><p>延迟可预测性。我们观察到，当在 GPU 上单独运行时（无干扰），DNN 推理中 GPU 内核的执行时间是确定性和可预测的。原因有两个。</p>
<ul>
<li>首先，内核主要是线性代数计算，例如矩阵乘法和卷积，既不包含条件分支，也不包含不恒定循环。</li>
<li>其次，所有内核参数（例如输入和权重）和输出都是固定大小的数组。</li>
</ul>
<p>因此，此类内核的执行时间与推理请求的输入无关，并且可以提前测量和准确预测。在实践中，我们观察到 DNN 模型的内核执行时间的方差通常只有几微秒而且非常正交。</p>
<h4 id="并行性"><a href="#并行性" class="headerlink" title="并行性"></a>并行性</h4><p>各种并行性。由于不同的输入规模，DNN 推理中的 GPU 内核通常表现出完全不同的并行性。例如，池化内核使用 64 个线程块，而 softmax 内核仅使用 1 个线程块。因此，DNN 推理的计算需求中需要的计算单元 (CU) 的数量，在执行过程中不断变化。因此，为了有效地利用 GPU，必须利用动态机制在运行时从不同的 DNN 推理任务中选择并执行多个内核。</p>
<h3 id="现有GPU调度state-of-art"><a href="#现有GPU调度state-of-art" class="headerlink" title="现有GPU调度state-of-art"></a>现有GPU调度state-of-art</h3><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101111943502.png" alt="image-20241101111943502"></p>
<ul>
<li>a-被动等待\顺序执行:大多数现有的 DNN 服务系统都使用顺序执行来避免任务之间的干扰。然而，由于抢占延迟（红色尺寸线）过长，RT 任务的端到端延迟可能会显着延长，因为它必须等待先前任务的完成（无抢占）。此外，由于顺序服务推理任务（即无并发性），该方案的总体吞吐量较差。</li>
<li>b-块级抢占:为了减少实时任务的端到端延迟，有必要抢占 GPU 运行尽力而为任务。然而，由于上下文较大（例如大量寄存器），很难在 GPU 上实现抢占式调度。同时，商用 GPU 也缺乏对抢占机制的硬件支持。先前的工作提出了基于等待的方法来实现 GPU 调度的块级抢占。实时任务仍然需要被动等待<strong>运行块</strong>完成。此外，抢占延迟将随着被抢占内核数量的增加而增加（见图1（c））。作为妥协，先前的工作 必须限制提交给 GPU 的内核数量，这对于 DNN 推理来说是不切实际的。此外，高频实时任务会破坏尽力而为任务的执行，甚至导致饥饿。</li>
<li>c-流级并行:为了提高整体吞吐量，现代 GPU 库（例如 CUDA [52] 和 ROCm [3]）通常提供多个 GPU 流。运行时调度程序按需从 GPU 流调度内核，以保持所有计算单元 (CU) 繁忙。虽然利用多个 GPU 流可以提高吞吐量，但并发任务可能会显着降低实时任务的延迟，延迟开销也会随着并发任务数量的增加而增加。(没有赋予RT任务更高的优先级,增加了吞吐量牺牲了延迟)</li>
</ul>
<h2 id="REEF结构"><a href="#REEF结构" class="headerlink" title="REEF结构"></a>REEF结构</h2><h3 id="总结构"><a href="#总结构" class="headerlink" title="总结构"></a>总结构</h3><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101112605339.png" alt="image-20241101112605339"></p>
<h4 id="DNN-模型准备（离线）"><a href="#DNN-模型准备（离线）" class="headerlink" title="DNN 模型准备（离线）"></a>DNN 模型准备（离线）</h4><p>通常，DNN 模型首先针对加速器后端（例如 GPU）进行编译和优化，然后加载到模型池中。受先前工作 [12,36,77] 的启发，REEF 使用代码转换器模块扩展了模型编译器（例如 TVM [15]），该模块首先<strong>验证</strong> DNN 模型中内核的幂等性，然后<strong>转换</strong>源代码以协助REEF 中的 GPU 调度。[how?]此外，REEF 开发了一个内核分析器来测量模型每个内核的计算要求和执行时间，这对于 DNN 模型来说是准确且实用的。</p>
<h4 id="DNN推理服务-在线"><a href="#DNN推理服务-在线" class="headerlink" title="DNN推理服务(在线)"></a>DNN推理服务(在线)</h4><p> REEF 通过四个主要组件扩展了最先进的 GPU 运行时（例如 ROCm [3]），用于 DNN 推理服务。</p>
<ul>
<li><strong>Task Queue</strong> REEF 维护一个实时任务队列和多个尽力而为Task Queue。每个队列都绑定到用于启动 GPU 内核的 GPU 流，其中推理请求按 FIFO 顺序提供服务。为简单起见，REEF 一次执行一个实时请求。请注意，任何将整个 GPU 视为单个设备的调度策略（例如 EDF [10]）都可以被 REEF 用于实时请求[应该是也能用的意思?]。此外，REEF 为基于 DNN 的应用程序提供基于 RPC 的接口，以向任务队列传递推理请求。</li>
<li><strong>Scheduler</strong> REEF 中的调度程序对任务队列使用繁忙轮询，并将任务分配给关联的 GPU 流。对应是否有实时任务，REEF提供了两种执行模式，即实时模式和普通模式。调度器遇到RT任务时会从普通模式切换到实时模式，当RT队列为空时又切换回普通模式。</li>
<li><strong>preemption</strong> 在正常模式下，REEF 使用 GPU 运行时提供的多个 GPU 流同时服务来自不同任务队列的尽力而为任务。在实时模式下，REEF 首先使用抢占模块立即从所有正在运行的尽力而为任务中抢占 GPU），然后立即在 GPU 上启动实时任务。</li>
<li><strong>DKP</strong>。在实时模式下，在启动实时内核之前，DKP 模块将选择适当的尽力而为内核并将它们动态填充到实时内核（§5）。 REEF 将在 GPU 上执行填充后的内核以实现高吞吐量。请注意，<strong>best effort 内核将仅使用实时内核剩余的 GPU 资源</strong>。</li>
</ul>
<h3 id="基于重置的抢占"><a href="#基于重置的抢占" class="headerlink" title="基于重置的抢占"></a>基于重置的抢占</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><p>DNN 模型中的 GPU 内核大多是幂等的，这使得主动抢占成为可能——立即杀死 GPU 上所有正在运行的内核并在稍后恢复它们。</p>
<ul>
<li><p>首先，它避免保存和恢复 GPU 的大型上下文（例如，每个 CU 256 KB 寄存器文件)。</p>
</li>
<li><p>其次，无需等待所有正在运行的内核完成，这可能需要数百微秒。</p>
<p>然而，在商用 GPU 上实现基于重置的抢占之前，仍然存在新的挑战。除了在 GPU 上运行的内核之外，数百个已启动的内核缓冲在由 GPU 运行时维护的多个队列中。这对于隐藏内核启动时间并充分利用 GPU 的大规模并行性是必要的。然而，需要逐出所有已启动的内核确实很难在数十微秒内抢占 GPU。</p>
</li>
</ul>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101113847986.png" alt="image-20241101113847986"></p>
<p>上图 说明了 GPU 运行时和设备中启动的<strong>内核</strong>的生命周期。首先，调度程序启动推理任务的所有内核，并为每个任务指定一个 GPU 流。 GPU运行时维护一个链表[谁和谁?]，称为<strong>主机队列</strong>，用于每个 GPU 流来缓冲启动的内核。每个主机队列都有一个后台线程，它将缓冲的内核异步传输到称为<strong>设备队列</strong>的环形缓冲区，CPU 和 GPU 同时访问该环形缓冲区。 GPU的命令处理器将轮询所有设备队列以获取缓冲的内核，并最终将它们分派到计算单元。因此，推理任务启动的内核可能存在于三个地方，即主机队列（HQ）、设备队列（DQ）和计算单元（CU）。为了实现即时抢占，必须驱逐所有三个地方的内核。</p>
<h4 id="驱逐缓冲区内核"><a href="#驱逐缓冲区内核" class="headerlink" title="驱逐缓冲区内核"></a>驱逐缓冲区内核</h4><p>基于重置的方法需要主动从主机队列和设备队列中逐出所有缓冲的内核。</p>
<p>对于主机队列，重置它们很简单，使所有缓冲内核出队并回收内存，因为它们完全由 GPU 运行时控制。</p>
<p>然而，对于设备队列，GPU 运行时无法从设备队列中逐出缓冲的内核，因为 GPU 的命令处理器可以直接从设备队列中获取内核，从而导致数据争用和不可预测的结果[一边拿一边放]。此外，CPU 也不提供从设备队列中安全驱逐内核的方法。</p>
<ul>
<li>一个潜在的解决方案是通知GPU重新注册一个新的设备队列。然而，这会产生不可接受的延迟开销（例如，在我们的测试床上大约有 1 毫秒）。受可驱逐内核的启发，我们提出<strong>延迟驱逐</strong>来重置设备队列，而无需扩展 GPU 运行时间和硬件。 <strong>REEF的代码转换器预先在每个内核的开头注入一段代码，通过检查抢占标志来判断是否已被驱逐。当抢占标志为真时，内核将自动终止。因此，当抢占发生时，抢占模块会立即将GPU内存中的抢占标志设置为true。设备队列中缓冲的内核将像往常一样被获取并分派到 CU，但会立即终止。</strong></li>
</ul>
<p>我们的初始队列驱逐机制给抢占过程带来了不小的开销，抢占单个任务需要超过 500 μs。深入分析表明，开销主要来自（a）从主机队列回收内存和（b）等待从设备队列获取内核。因此，我们提出两种优化来减轻开销。</p>
<ul>
<li>异步内存回收。当对主机队列中被逐出的内核使用同步内存回收时，抢占延迟与主机队列长度成正比。因此，抢占 DNN 推理任务的性能损失将是巨大的，因为它需要在主机队列中缓冲数百个内核。为了立即从主机队列中逐出 GPU 内核，REEF 利用<strong>后台 GC 线程异步回收内存。</strong>具体来说，REEF通过简单地先将头指针置空，然后通知GC线程在后台回收内存来重置主机队列。</li>
<li>设备队列容量限制。尽管使用延迟逐出可以在执行开始时立即终止设备队列中的内核，但仍然需要获取内核并将其分派到 CU，每个内核大约需要 20 μs。在设备队列中缓冲数百个内核是很常见的，因为它可以通过一次用来自主机队列的大量内核填充设备队列来减少上下文切换的频率。然而，它也可能将抢占延迟增加到甚至超过 1 ms。因此，REEF通过限制设备队列的容量来实现微秒级的内核抢占。<strong>调整设备队列容量可以在抢占延迟和执行时间之间进行权衡。随着队列容量的减少，抢占延迟也会减少，因为需要驱逐的内核更少，但正常执行时间会增加，因为 GPU 有更多的空闲时间等待运行时用主机队列中的内核填充设备队列。</strong>[用减少驱逐内核的方式牺牲运行时间减少内存驱逐的影响]我们根据经验在测试台上选择设备队列容量为 4，因为在 30 μs 内重置设备队列就足够了，而正常执行时间的开销可以忽略不计（即小于 0.3%）。此外，由于设备队列的填充更加频繁，使用较小的设备队列还会产生稍高的 CPU 利用率（例如，增加约 15%）。</li>
</ul>
<h4 id="杀死正在运行的内核"><a href="#杀死正在运行的内核" class="headerlink" title="杀死正在运行的内核"></a>杀死正在运行的内核</h4><p>为了避免等待正在运行的内核完成，基于重置的抢占会主动终止 GPU 中正在运行的内核。不幸的是，GPU 运行时提供的 API 和 GPU 驱动程序都没有提供可以从主机端终止正在运行的内核的功能。我们观察到 GPU 驱动程序能够终止 CPU 进程并杀死相关的 GPU 内核，即使内核陷入无限循环也是如此。这意味着 GPU 驱动程序确实可以杀死未完成的内核。但是，该函数也会回收进程和 GPU 内核分配的 GPU 内存。因此，被抢占的内核必须将 DNN 模型参数重新加载到 GPU 内存，甚至需要几秒钟的时间。</p>
<p>为了解决这个问题，<strong>REEF改进了GPU驱动程序的内核杀死功能，并将其暴露给GPU运行时的抢占模块。新函数将指示命令处理器终止 CU 上所有正在运行的内核，但将其运行状态保留在 GPU 内存中</strong>。在驱逐主机队列和设备队列后，抢占模块将使用它来杀死所有正在运行的内核。</p>
<h4 id="恢复抢占任务"><a href="#恢复抢占任务" class="headerlink" title="恢复抢占任务"></a>恢复抢占任务</h4><p>尽力而为的任务应该在被抢占后恢复。一般来说，任务必须从头开始重新执行，并且假设没有副作用。幸运的是，DNN 模型中内核的幂等特性保证了 DNN 推理任务的执行可以从中断内核之前的任何内核恢复。这意味着调度程序可以安全地重新执行抢占的尽力而为任务。然而，这可能会产生严重的额外开销，因为 DNN 模型通常具有大量内核（通常为数百个或更多）。因此，将被抢占的任务从内核中恢复到中断位置附近非常重要。</p>
<p>不幸的是，精确识别中断的内核几乎是不可能的，因为运行在CU上的内核会被GPU的命令处理器直接杀死。为了解决这个问题，抢占模块在开始重置任务队列时首先记录最后一个传输到设备队列的内核（kl），然后从kl之前的c个内核中恢复被抢占的任务，其中c表示设备队列容量。[就算整个设备队列都是满的也能确保都是没有被运行过的,而且设备队列很小!!]我们观察到命令处理器顺序地从设备队列中获取内核并在 CU 上运行它。这意味着被中断的内核不会早于设备队列中最后一个内核（kl）之前的 c 个内核。此外，REEF 将冗余执行最多 c+1 个内核。<strong>由于 c 配置得相对较小（即 4）</strong>，因此恢复开销可以忽略不计（大约 30 μs）。</p>
<h4 id="抢占在闭源GPU上的效果"><a href="#抢占在闭源GPU上的效果" class="headerlink" title="抢占在闭源GPU上的效果"></a>抢占在闭源GPU上的效果</h4><p>许多商用 GPU（例如 NVIDIA GPU）仍然是闭源的。这对我们基于重置的抢占方案提出了新的挑战，该方案必须将 GPU 运行时视为黑匣子。主要限制是我们无法重置 CU 来主动终止正在运行的内核。除此之外，REEF 还无法在 GPU 运行时之外直接操作主机队列和设备队列。但幸运的是，REEF 提出的用于重置 DQ 的延迟驱逐方案不需要对 GPU 运行时进行任何修改。</p>
<p>我们为闭源 GPU 提出了基于重置的抢占的受限版本，称为 REEF-N。 REEF-N 首先将每个 GPU 流（GPU 运行时提供的一般抽象）包装到虚拟主机队列 (vHQ) 中，该队列拦截并缓冲所有启动的内核。与 GPU 运行时内的（物理）HQ 类似，每个 vHQ 也有一个后台线程将缓冲内核异步传输到 GPU 运行时。<strong>之后，REEF-N 将整个 GPU 运行时视为多个设备队列（每个 GPU 流一个）</strong>[什么意思]，这样 REEF 可以轻松重置 vHQ 以逐出缓冲内核，而不是直接重置 HQ（如图 7 所示）。 REEF-N 仍然遵循延迟驱逐来重置 DQ，然后等待所有正在运行的内核完成。最后，为了模拟DQ容量限制，REEF限制了GPU运行时中未完成的内核的数量； vHQ 的后台线程在闭环中将固定数量的内核传输到 GPU 运行时。</p>
<h3 id="动态内核填充"><a href="#动态内核填充" class="headerlink" title="动态内核填充"></a>动态内核填充</h3><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101143138816.png" alt="image-20241101143138816"></p>
<p>为了实现高吞吐量，实时任务和尽力而为任务应该在 GPU 上同时执行。但是，为了避免干扰实时任务，应仅使用实时任务剩余的 GPU 资源来服务尽力而为的任务。遗憾的是，现有的方法都无法在 GPU 上提供这种受控并发执行。</p>
<ul>
<li>首先，使用不同的GPU流来启动实时且尽力而为的任务无法避免相互干扰。如上图所示，GPU 流之间的调度延迟 (20-40 μs) 可能会推迟实时内核的执行或限制它们的可用资源（例如 CU）。使用额外的流间屏障来同步 CU 之间的内核调度也会导致性能开销。</li>
<li>其次，静态内核融合可以在编译时将来自不同任务的多个内核合并为一个内核，然后使用单个流在 GPU 上启动融合内核。它可以提前避免实时任务和尽力而为任务之间的干扰。然而，静态内核融合必须预编译 DNN 模型中所有内核的所有可能组合，以便在运行时进行调度。如上所述，DNN 推理有数百个共同的内核，这使得静态内核融合不切实际。例如，仅考虑不超过三个内核的所有组合,它需要超过 35 GB 的 GPU 内存来存储表 1 中 DNN 模型的融合内核.</li>
</ul>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101143637626-1730443000309-1.png"></p>
<p>我们的方法：动态内核填充。受内核融合的启发，我们的方法还将实时内核和尽力而为内核合并为一个，并使用单个 GPU 流启动它，如图 8 所示。不同的是，我们在以下位置构建一个模板（称为 dkp 内核）：编译时并使用函数指针在运行时填充和执行内核。此外，我们动态选择尽力而为的内核以避免干扰实时内核。图 9 显示了用于动态内核填充的 dkp 内核 (dkp) 的示例，声明为全局函数（即内核条目）。候选内核函数（例如，dense）不是静态内联到 dkp 内核中，而是被声明为单独的设备函数，可以作为 dkp 内核参数传递并由函数指针调用（第 3 行和第 8 行）[??]。 dkp 内核对 CU 进行分区，以并行执行一个实时候选内核 (rt_kern) 和一组尽力而为的候选内核 (be_kerns)。它首先为实时内核分配足够的 CU（第 1-3 行），然后将剩余的 CU 分配给尽力而为内核（第 5-8 行）。当启动实时内核时，DKP 模块会选择适当的尽力而为内核与实时内核同时执行。</p>
<h4 id="高效的函数指针"><a href="#高效的函数指针" class="headerlink" title="高效的函数指针"></a>高效的函数指针</h4><p>如果没有特定的优化，由于 GPU 上函数指针的独特特性，我们的设计将降低实时内核的性能。我们总结了GPU上默认函数指针机制的两个关键性能问题。</p>
<ul>
<li><p><strong>有限的寄存器分配。</strong>与 CPU 程序不同，GPU 程序需要不同但固定数量的寄存器，这些寄存器在编译时进行计数并编码到模型可执行文件中。而因为间接调用的函数使用的寄存器数量无法静态确定,这样的特性禁止在 GPU 内核中直接使用函数指针。 GPU编译器的默认行为是分配一个预定义的静态上限来限制被调用者的寄存器使用，这可能会由于寄存器不足而迫使被间接调用者将变量保存在堆栈上，从而导致与纯粹使用寄存器相比性能较差。</p>
</li>
<li><p><strong>昂贵的上下文保存。</strong> GPU 上的间接函数调用比 CPU 程序昂贵得多，因为在函数调用之前和之后需要保存和恢复大量的上下文（例如，数十个寄存器）。对于数千个线程，可能会保存和恢复 MB 大小的寄存器，从而引入大量开销。尽管编译器将内联[??]尽可能多的函数以避免这种开销，但无法内联通过函数指针的间接函数调用，这可能会对动态内核填充造成显着的性能损失。 </p>
<p>REEF 通过引入全局函数指针[所以谁是全局函数指针]来替代默认函数指针机制来解决上述两个问题。由于全局函数被视为内核条目，因此编译器既不应用寄存器限制，也不向它们添加上下文保存&#x2F;恢复代码[不需要吗??]。因此，将候选内核声明为全局函数而不是设备函数可以解决这两个问题。根据我们的观察，候选内核中的上下文保存实际上是不必要的，因为 dkp 内核在调用 rt_kern 或 be_kerns[i] 后立即退出（见图 9）[为啥啊??]。因此，候选内核中缺少上下文保存代码不会影响执行的正确性。然而，作为内核入口，全局函数不能被另一个全局函数（例如，dkp内核）调用。为了绕过这个限制，我们用汇编代码中的跳转指令替换间接函数调用，并按照约定手动准备候选内核的初始状态[45]。这种方法不会对编译器进行任何更改，并且只会产生微不足道的函数调用开销（大约 1%）。[主要逻辑-寄存器上下文保存无法恢复+寄存器有静态上限-&gt;全局函数(因为候选内核不受编译器寄存器限制不需要进行上下文保存,调用后立即退出-&gt;全局函数不能被另一个全局函数调用-&gt;使用汇编的跳转指令替换函数调用(how??)并且手动准备候选内核的初始状态)]</p>
</li>
</ul>
<p>其他的优化方法包括:</p>
<ul>
<li>动态寄存器分配。应用全局函数指针技术后，由于存在过度分配问题，实时内核性能仍然不理想。为了满足候选内核不同的寄存器需求，<strong>dkp内核必须分配尽可能多的寄存器（即过度分配），这可能会减少CU占用，从而增加执行时间</strong>。[CU占用率意味着一个CU上可以同时执行多少个块。这取决于每个块需要多少资源（例如寄存器）。更高的 CU 占用率可以带来更好的性能。]一个直观的解决方案是在启动之前及时覆盖 dkp 内核的寄存器计数，使其适应选定的候选内核。不幸的是，内核的寄存器计数在离线阶段已随模型一起加载到 GPU 内存中，这意味着覆盖其值需要在每次内核执行之前进行 CPU 到 GPU 内存复制，严重影响执行性能.<strong>为解决寄存器过度分配的问题,REEF引入一组代理内核</strong>。代理内核与图 9 中的 dkp 内核共享相同的源代码，但分配不同数量的寄存器，允许调度程序根据每个候选内核的寄存器需求动态选择合适的代理内核。不幸的是<strong>，为每个可能的寄存器计数生成代理内核面临着内核数量爆炸的问题</strong>。例如，在每个线程最多有 128 个标量寄存器和 256 个向量寄存器的 AMD Instinct MI50 GPU 上，它将生成 32,768 个代理内核以覆盖所有可能的寄存器配置。<strong>为了减少代理内核数量，我们生成代理内核来覆盖所有可能的 CU 占用而不是寄存器计数[二者有什么关系?没懂]。</strong>由于引入代理内核是为了防止过度分配而减少 CU 占用率，因此具有不同寄存器数量但共享相同 CU 占用率的代理内核实际上是冗余的，可以合并在一起。更具体地说，我们使用的 AMD Instinct MI50 GPU 上有 10 个 CU 占用级别，对应 10 个寄存器计数范围，这使得我们只能生成 10 个代理内核，每个代理内核分配一个 CU 占用级别允许的最大寄存器数量。对于每个候选内核，调度程序都会选择分配寄存器最少的代理内核来满足候选内核的需求，从而实现尽可能高的 CU 占用率。这样，代理内核的数量从 32,768 个缩小到 10 个，而不会影响候选内核的性能。</li>
<li>动态共享内存。除了寄存器之外，共享内存的过度分配也可能会降低代理内核的 CU 占用率。幸运的是，内核可以通过在启动内核时设置一个属性（即“动态共享内存”）来动态分配共享内存。在模型编译过程中，REEF 将变量的声明从固定大小共享内存转换为动态共享内存（即在 <strong>shared</strong> 之前添加 extern）。因此，代理内核使用的共享内存量可以在运行时设置，具体取决于候选内核的最大需求。</li>
</ul>
<h4 id="内核选择"><a href="#内核选择" class="headerlink" title="内核选择"></a>内核选择</h4><p>对于动态内核填充，内核选择策略对于避免实时任务的延迟干扰非常重要，实时任务会从候选尽力而为内核中选择一组块[内核的粒度大于块?]，以便与到达的实时内核共享 GPU。 REEF 提出了一种贪婪启发式方法，以确保尽力而为的块将仅使用实时内核剩余的 GPU 资源（即 CU）。具体来说，它首先为实时内核预留足够的CU，然后检查尽力而为任务队列，为剩余的CU选择合适的块，直到没有空闲的CU或候选任务。所选的尽力而为块应满足以下两个规则。</p>
<ul>
<li>规则1.尽力而为内核的执行时间必须比实时内核的执行时间短，因为dkp内核的执行时间是由最慢的块决定的。基于对 DNN 模型中 GPU 内核延迟可预测性的观察（参见第 2.1 节），我们开发了一个离线内核分析器来测量加载模型的每个内核的计算要求和执行时间。[BE不能耽误RT]</li>
<li>规则2.尽力而为内核的CU占用率必须高于实时内核的CU占用率，因为dkp内核的CU占用率是由最小内核决定的。请注意，内核的 CU 占用率可以直接从 DNN 模型的源代码中获得。[内核越小\占用率越高,这个目的是保证RT有更多寄存器?被分配到更大的内核?有必要吗]</li>
<li>内核选择策略完全满足将实时任务视为GPU上的一等公民的设计目标。它不仅高效，可以在不到 1 μs 的时间内选择尽力而为的内核，而且也很有效，将实时内核的延迟开销平均限制在 1% 以下。然而，该策略也是保守的，因此该约束可能会限制整体吞吐量的改进空间。例如，<strong>当尽力而为内核的执行时间通常比实时内核长时，动态内核填充的吞吐量改进可能微不足道，即使RT任务仅使用几个 CU。</strong></li>
</ul>
<h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>我们首先在 AMD GPU 上实现并部署 REEF，因为它的开源平台和 ISA [26, 54]，它可以充分展示基于重置的抢占和动态内核填充的功效。 REEF是通过扩展Apache TVM [73]和AMD ROCm [3]来实现的，大约有5,500行C++代码。除此之外，为了进一步展示 REEF 在闭源 GPU 上的可行性，我们还在带有 CUDA 的 NVIDIA GPU 上移植了 REEF-N（基于重置的抢占的受限版本）[52]。</p>
<p>模型编译器: REEF 通过代码转换器扩展了机器学习编译框架 Apache TVM [15]，主要对 DNN 推理的源代码添加了两处修改：（1）抢占标志，将其注入到内核参数中以延迟逐出内核; (2)一组代理内核，它是为内核填充而构建的。</p>
<p>GPU 运行时:</p>
<ul>
<li>对于 AMD GPU，REEF 在 ROCm（一个便携式 GPU 运行时和编程库）的 HIP [63] 上构建了抢占模块。类似于 NVIDIA CUDA [52]。具体来说，REEF为GPU运行时添加了三个新的API：（1）hip_reset_hq，它重置主机队列并将USENIX协会第16届USENIX操作系统设计与实现研讨会54条命令移至GC线程； (2) hip_set_stream_cap，限制一个GPU流使用的设备队列的容量； (3) hip_reset_kern，它通过Linux中的GPU驱动程序使用硬件机制重置计算单元[61]。</li>
<li>对于NVIDIA GPU，REEF-N拦截了与内核启动和流管理相关的三个CUDA API，并添加了以下操作：（1）cuStreamCreate，它创建vHQ并将其链接到创建的CUDA流； （2）cuKernelLaunch，它在vHQ中缓冲启动的内核并将其传输到后台的GPU运行时（即CUDA [52]）； (3) cuStreamSynchronize，等待GPU运行时完成CUDA流的所有启动内核。最后，REEF-N 提供了一个新的 API cuResetHQ，通过使所有缓冲内核出队来重置 vHQ。</li>
</ul>
<h3 id="实验基础"><a href="#实验基础" class="headerlink" title="实验基础"></a>实验基础</h3><h4 id="测试条件"><a href="#测试条件" class="headerlink" title="测试条件"></a>测试条件</h4><p>试验台。实验主要在 GPU 服务器上进行，该服务器由 1 个 Intel Core i7-10700 CPU（共 8 核）、16 GB DRAM 和 1 个 AMD Radeon Instinct MI50 GPU（60 个 CU 和 16GB 内存）组成。服务器的软件环境配置为ROCm 4.3.0 [3]、Apache TVM [73] 0.8.0和Ubuntu 18.04。硬件平台类似于自动驾驶汽车的计算资源 [4, 71]。我们使用安装了 CUDA 10.2 [52] 的同一服务器，在闭源 GPU（NVIDIA V100 GPU）上进一步评估 REEF-N，以证明我们方法的通用性。</p>
<h4 id="负载条件（DISB）"><a href="#负载条件（DISB）" class="headerlink" title="负载条件（DISB）"></a>负载条件（DISB）</h4><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101191640539.png" alt="image-20241101191640539"></p>
<p>工作负载。受 YCSB的启发，我们构建了一个新的 DNN 推理服务基准（DISB），其中包含一套工具和五个工作负载：（A）低负载，（B）高 RT 负载，（C）高 BE 负载， (D) 多 RT 负载和 (E) 随机负载，如表 2所示。</p>
<p>DISB A–D 中的实时 (RT) 客户端统一以给定频率发送推理请求，这模拟了实时 DNN 应用程序自动驾驶（例如，使用摄像头进行障碍物识别），而 DISB E 中的客户端每秒发送 20 个具有泊松到达分布的请求，模拟事件驱动的实时 DNN 应用。请注意，对于 VGG 模型，每秒顺序服务 220 个 RT 请求将使我们的测试床饱和。另一方面，尽力而为（BE）客户端不断发出推理请求，模拟 GPU 上的争用负载（例如驱动程序监控）。 DISB 中部署了五个代表性的 DNN 模型，包括 ResNet-152 [30] (RNET)、DenseNet-201 [35] (DNET)、VGG-19 [68] (VGG)、Inception v3 [69] (IN3) 和DistilBert [66] (BERT)，全部由 Apache TVM [15] 生成。每个客户端总是提交针对某个 DNN 模型的推理请求。五类模型部署在5类DISB上。</p>
<p>此外，我们使用来自开放自动驾驶平台（即 Apollo）的真实世界轨迹作为实时工作负载，这提供了自动驾驶中实时任务的真实到达分布。我们从上述五个模型中选择了执行时间最接近的 DNN 模型来进行推理请求。同时，使用与 DISB C-E 相同的尽力而为的工作负载，其中五个客户端连续发出不同的 DNN 推理请求。</p>
<h4 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h4><p>我们将 REEF 与典型的调度方法进行比较。 SEQ 通过被动任务抢占在 GPU 上顺序运行每个 DNN 推理任务，Clockwork [28] 采用了这种方式。具体来说，当队列中有多个任务等待时，它会优先考虑实时任务，但仍然需要等待已启动的besteffort任务完成。 GPUStreams 通过多个 GPU 流在同一 GPU 上同时运行实时任务和尽力而为任务，这一点被 TensorRT [50] 采用。作为参考，我们进一步提供 RT-Only，它代表实时任务的最佳端到端延迟，因为它将 GPU 专用于实时任务。</p>
<h4 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h4><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101194812969.png" alt="image-20241101194812969"></p>
<h5 id="单个BE（DSIB-A-B"><a href="#单个BE（DSIB-A-B" class="headerlink" title="单个BE（DSIB A&#x2F;B)"></a>单个BE（DSIB A&#x2F;B)</h5><p>对具有单个 BE 客户端的工作负载，使用 SEQ 或 GPUStreams 对性能的影响相对较低，因为尽力而为任务的 GPU 争用并不严重，无论是在等待时间 (SEQ) 还是并发干扰 (GPUStreams) 方面。对于DISB A，与 RT-Only 相比，<strong>SEQ 和 GPUStreams 将整体吞吐量提高了 1.46 倍和 1.66 倍，但也将实时任务延迟分别放大了 1.95 倍和 1.84 倍。相比之下，REEF 在实时任务延迟方面产生的开销可以忽略不计 (0.5%)，但与 GPUStreams 相比，总体吞吐量提高了 1.60 倍。</strong></p>
<p><strong>对于 DISB B，由于运行实时任务更频繁，SEQ 的实时任务延迟降低了 1.12 倍，略好于 DISB A，因为它只需要等待</strong>更少的尽力而为的任务。然而，它的吞吐量仅达到 RT-Only 的 96%，因为实时任务使 GPU 饱和，而尽力而为任务几乎没有机会运行。出于类似的原因，GPUStreams 的整体吞吐量也下降至 RT-Only 的 76%，而其实时任务延迟仍比 RT-only 高 1.70 倍。相反，得益于我们基于重置的内核抢占和动态内核填充，REEF 仍然可以将实时任务延迟的开销限制在 1%（约 60 μs），并在整体吞吐量上提供 1.14 倍的加速。</p>
<p>对于单个BE的的任务，SEQ和GPUStream为了延迟牺牲的吞吐量多，但Reef是一个均衡。</p>
<h5 id="多个BE（DSIB-C-D-E"><a href="#多个BE（DSIB-C-D-E" class="headerlink" title="多个BE（DSIB C&#x2F;D&#x2F;E)"></a>多个BE（DSIB C&#x2F;D&#x2F;E)</h5><p>多个 BE 客户端（DISB C、D 和 E）。随着尽力而为工作负载的增加，通过在两种类型的任务之间共享 GPU，所有方法的整体吞吐量比 RT-Only 都有不同程度的提高。然而，它们在实时任务延迟方面的表现却截然不同。 SEQ 和 GPUStream 在实时任务延迟和总体吞吐量之间做出了相同的权衡，只是性能影响的程度不同。对于三个工作负载，SEQ 将整体吞吐量提高了 1.34 倍至 2.10 倍，但也将实时任务延迟放大了 1.51 倍至 1.86 倍。对于 GPUStreams，上述数字变为 3.94× 至 8.19× 和 2.65× 至 3.31×。</p>
<p>不同的是，REEF 在不影响实时任务的前提下，尽可能提高整体吞吐量。因此，REEF 在所有工作负载中提供与 RT-Only 几乎相同的实时任务延迟，且开销不到 1.5%（0.1 毫秒）。对于总吞吐量，因为 VGG 很容易在大多数 DNN 模型被填充，REEF 在 DISB C 上提供了 GPUStreams 的接近结果。在 DISB D 和 E 上，REEF 的吞吐量比 GPUStreams 低约 25%，这是由于混合使用了五种 DNN 模型来执行实时任务，而 DKP 在少数RT和BE的任务组合上并不总是能很好地工作。然而，REEF 的性能仍分别优于 RT-Only 3.00 倍和 2.96 倍。</p>
<p>对于单个BE的的任务，SEQ和GPUStream为了吞吐量牺牲的延迟多（RT-ONLY都直接不管BE任务了），但Reef是一个均衡。</p>
<h5 id="真实世界"><a href="#真实世界" class="headerlink" title="真实世界"></a>真实世界</h5><p>对于现实世界的工作负载，与 RT-Only 相比，SEQ 和 GPUStreams 将整体吞吐量提高了 3.6 倍和 8.3 倍，同时将实时任务的延迟分别放大了 1.35 倍和 3.35 倍。由于现实世界跟踪中实时任务的负载较低（大约 43 个请求&#x2F;秒），REEF 大部分时间都保持在正常模式下并发执行尽力而为的任务，类似于 GPUStreams。因此，与 RT-Only 相比，REEF 实现了 7.7 倍的吞吐量提升，实时任务的延迟开销低于 2%，这要归功于我们基于重置的抢占，它可以在实时任务到达后的数十微秒内抢占 GPU。</p>
<h4 id="DNN推理抢占结果"><a href="#DNN推理抢占结果" class="headerlink" title="DNN推理抢占结果"></a>DNN推理抢占结果</h4><p>之前的工作 [12] 中提出的基于等待的抢占方法对于 DNN 推理服务来说并不实用，因为它只允许一项一项地执行任务。因此，我们通过取消启动内核数量的限制并实现延迟驱逐来扩展它以允许并发推理服务，并将该版本用作基线来演示我们基于重置的抢占的效率。</p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101195250169.png" alt="image-20241101195250169"></p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101195343686.png" alt="image-20241101195343686"></p>
<p>效果很好且对模型、内核数目不敏感。</p>
<h5 id="优化效果"><a href="#优化效果" class="headerlink" title="优化效果"></a>优化效果</h5><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101201601290.png" alt="image-20241101201601290"></p>
<p>这两个图就是考虑不优化&#x2F;拆解优化的情况。</p>
<p>我们对基于重置的抢占方法提出了两种优化，即异步内存回收和队列容量限制。为了演示优化的效果，图 14 显示了BE 发送端 (RNET) 增加后的抢占延迟，以及单个 BE 客户端的延迟细分。通过启用两项优化，抢占延迟显着下降高达 92%（从 87%），如图 14(a) 所示。即使不优化Reef也很棒。</p>
<p>由于这两种优化分别在重置主机和设备队列时使用，因此图 14（b）分解了抢占延迟以分别显示两种优化的贡献。对于单个BE客户端，使用异步内存回收将重置主机队列的延迟从17μs减少到3μs。同时，利用队列容量限制进一步将重置设备队列的延迟从424μs减少到31μs。请注意，使用命令处理器重置 CU 的速度非常快（小于 3 μs）。【看不出来，w&#x2F;o是什么意思】</p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101202356613.png" alt="image-20241101202356613"></p>
<p><strong>队列容量选择。</strong>我们限制设备队列容量，以减轻因延迟驱逐队列中剩余内核而产生的开销。但是，减少队列容量也会增加正常执行时间和 CPU 利用率。图 15(a) 显示当设备队列容量从1增加到4时，执行时间从14.3 ms减少到12.3 ms。然而，当容量进一步增加时，执行时间的变化变得微不足道（小于 0.3%）。相反，抢占延迟随着队列容量线性增加。因此，作为抢占延迟和正常执行时间之间的合理权衡，REEF在我们的测试平台上对设备队列采用了默认容量4，这对于正常执行来说几乎为零的开销，并提供了可接受的抢占性能（约30μs）。最后，使用较小的设备队列还会导致较高的 CPU 利用率【怎么算的】。例如，将队列容量从 256 个减少到 4 个，CPU 利用率从 17% 增加到 31%。</p>
<p><strong>任务恢复。</strong>我们进一步评估由于任务恢复而被抢占的任务的执行时间开销。我们使用单个 BE 客户端发送推理请求；对于每个任务，我们随机抢占并恢复它。如图15（b）所示，所有DNN模型的恢复时间都很短，从70μs到245μs不等，这主要取决于DNN模型的内核执行时间（见图10）。请注意，由于队列容量限制，REEF 最多冗余执行 5 个内核来恢复抢占的任务。此外，除了 VGG (5.1%) 之外，所有 DNN 模型的执行时间开销约为 2%，因为它的内核最少 (55)，而且它的内核执行时间更长。【Overhead是占哪个的比】</p>
<h4 id="动态内核填充-1"><a href="#动态内核填充-1" class="headerlink" title="动态内核填充"></a>动态内核填充</h4><p>为了研究动态内核填充的功效，我们使用高争用工作负载，其中一个 RT 客户端和一个 BE 客户端同时以足够高的频率发送请求以使 GPU 保持忙碌。 RT-Only 仅服务于实时任务，以确保最佳（实时）任务延迟，而 GPUStreams 同时服务于两种类型的请求，以实现最高的总体吞吐量。不同的是，动态内核填充也仅服务于实时任务，但会填充尽力而为的任务以避免饥饿并提高整体吞吐量。</p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101203419207.png" alt="image-20241101203419207"></p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101203647916.png" alt="image-20241101203647916"></p>
<h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>C55填充</p>
<p>图 16 报告了使用上述工作负载的五个 DNN 模型之间一对一组合的实验结果。正如预期的那样，由于并发尽力而为任务的严重干扰，GPUStreams 使实时任务延迟平均显着增加了 1.35 倍，范围从 1.04 倍到 1.70 倍。然而，REEF 能够为实时任务提供几乎最优的延迟，平均开销仅为 1%（最高 3%）。</p>
<p>对于 RT-Only，GPU 正忙于服务实时任务，因此尽力而为任务的吞吐量为零（即使 RT-Only 愿意服务它们）。虽然GPUStreams整体吞吐量平均提升1.52倍，但由于并发执行干扰严重，实时任务吞吐量平均下降24.4%。相反，REEF 首先保证实时任务的吞吐量，然后利用动态内核填充来提高整体吞吐量。</p>
<p>性能的提升主要取决于两个条件。首先，GPU上实时任务的执行还有改进的空间。如图 17 所示，IN3 和 BERT 中的实时内核平均分别使用 85% 和 70% 的 CU。【图17】因此，动态内核填充很难改善这种情况，平均仅增加 6%。请注意，GPUStreams 仍然可以提高它们的整体吞吐量，但也极大地牺牲了实时任务的性能。其次，尽力而为内核的执行时间必须比填充的实时内核的执行时间短。这解释了为什么REEF通过用RNET填充VGG可以实现很大的改（1.41×），但反之则不然，这也被图17中CU使用率（BE）的增加所证实。</p>
<h5 id="优化检验"><a href="#优化检验" class="headerlink" title="优化检验"></a>优化检验</h5><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101203845199.png" alt="image-20241101203845199"></p>
<p>为了研究优化对性能和内存使用的影响，<strong>我们首先使用 GPU 上函数指针的不同实现来评估开销</strong>。我们通过 dkp 内核启动实时内核来测量此类开销，而不填充任何尽力而为的内核。如图 18(a) 所示，对于具有不同 DNN 模型的实时任务，默认函数指针实现（Default）会产生从 78% 到 503% 的执行时间开销。通过使用全局函数指针（GlobalPtr），开销平均显着降低至 46.4%（从 11.5% 降至 120%），因为它消除了设备函数指针寄存器数量的限制，并避免了额外的寄存器保存和恢复在函数调用期间。最后，通过使用代理内核（ProxyKernel），开销平均下降到0.8%（最多1.21%），它可以动态地为每个内核分配寄存器，并最大化CU占用。最小的开销来自CU分区的逻辑分支和全局函数指针的初始状态准备。</p>
<p>如图18（b）所示，使用静态内核融合（Kernel Fusion）需要超过35 GB的GPU内存来存储五个DNN模型的融合内核——所有组合不超过三个内核，甚至超出了内存容量大多数商品 GPU 的。 REEF 提出代理内核（DKP w&#x2F;o OPT）以将 GPU 内存使用量减少到约 32 MB。最后，生成代理内核来覆盖所有可能的 CU 占用（DKP w&#x2F; OPT），而不是所有可能的寄存器配置，可以将 GPU 内存使用量大幅减少到仅 10 KB。</p>
<p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101203958105.png" alt="image-20241101203958105"></p>
<p><strong>内核选择。</strong>图 19(a) 显示了动态内核填充期间 DISB A-E 的内核选择的平均时间。对于具有单个 BE 客户端（DISB A 和 B）的工作负载，REEF 需要大约 0.2 μs 来为给定的实时内核选择尽力而为的内核。对于具有多个 BE 客户端（DISB C、D 和 E）的工作负载，由于候选者增多，选择时间增加到 0.4 μs。一般来说，内核选择的成本相当微不足道，并且可以很容易地被内核执行隐藏。为了进一步研究内核选择的准确性，我们评估了由于在所有 DISB 工作负载上填充尽力而为内核而导致的实时内核的执行时间开销。如图19（b）所示，超过37％的实时内核没有受到尽力而为内核并发执行的负面影响，并且超过90％的实时内核的开销仍然小于4μs。执行时间的增加主要是由于GPU内存和共享L2缓存的争用。</p>
<h4 id="闭源GPU"><a href="#闭源GPU" class="headerlink" title="闭源GPU"></a>闭源GPU</h4><p><img src="/./../images/Microsecond-scale-Preemption-for-Concurrent-GPU-accelerated-DNN-Inferences%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241101204036888.png" alt="image-20241101204036888"></p>
<p>最后，我们评估 REEF-N，这是在 NVIDIA 和 AMD GPU 上使用 DISB 工作负载的基于重置的抢占的受限版本，并将其分别与基于等待的方法和 REEF 进行比较。如图 20 所示，即使 REEF-N 不重置 CU 来主动终止正在运行的内核，抢占延迟也仅为 71μs 至 288μs，这仍然比基于等待的方法高出 12.3 倍（从 6.3 倍）在 NVIDIA GPU 上。通过比较 AMD GPU 上的 REEF-N 和 REEF，我们观察到主动终止正在运行的内核进一步有助于抢占延迟平均加速 2.0 倍，特别是对于抢占并发任务（例如 DISB C 为 2.3 倍）。此外，REEF-N 在两个 GPU 上的性能接近。</p>
<h3 id="研究不足"><a href="#研究不足" class="headerlink" title="研究不足"></a>研究不足</h3><h4 id="幂等性假设"><a href="#幂等性假设" class="headerlink" title="幂等性假设"></a>幂等性假设</h4><p> REEF 中基于重置的抢占基于 DNN 推理中的每个内核应该是幂等的假设。目前，我们遇到的所有 DNN 推理内核（来自 11 个模型 [72] 的总共 320 个内核）都被证明是幂等的。然而，读者可能会对我们的方法是否仍然适用于没有幂等假设的内核感兴趣。严格来说，基于重置的抢占要求内核始终为相同的输入产生相同的输出，无论是否已重试。因此，如有必要，可以使用事务化方法将非幂等内核转换为幂等内核。</p>
<p>此外，由于在 REEF 中只有尽力而为的内核可以被抢占，因此这种转换只会牺牲转换后的内核（即尽力而为的内核）的性能，以确保实时内核可以在到达时立即执行，而不会造成性能损失。我们将这项技术的结合留到未来的工作中，直到我们真正遇到非幂等 DNN 内核。</p>
<h4 id="对内核选择的限制。"><a href="#对内核选择的限制。" class="headerlink" title="对内核选择的限制。"></a>对内核选择的限制。</h4><p>当前的内核选择策略是有效但保守的，因为 REEF 的主要目标是避免对实时任务的性能干扰。一个明显的限制是尽力而为内核的执行时间必须比填充的实时内核的执行时间短，这限制了整体吞吐量的改进空间。我们发现，通过在模型编译期间使用更多线程块，可以对 GPU 内核进行定制，以缩短每个块的执行时间。例如，Apache TVM 自动调整线程块的数量以提高整体性能，但也允许开发人员对其进行自定义[38]。目前，REEF 整体吞吐量的提高很大程度上归功于启用即时内核抢占，它允许空闲的 GPU 执行尽力而为的任务。因此，我们将其留给未来的工作来克服内核选择的限制。此外，该策略没有考虑实时内核和尽力而为内核之间的 GPU 内存争用，因为它仍然足以运行多个 DNN 推理任务。我们也把它留给未来的工作。</p>
<h4 id="未来的GPU-API和运行时"><a href="#未来的GPU-API和运行时" class="headerlink" title="未来的GPU API和运行时"></a>未来的GPU API和运行时</h4><p>我们为未来的GPU提供了设计借鉴：使用单独的 GPU API 来精确重置 CU 是可行的，并且对于终止和恢复所有正在运行的内核很有用。其次，我们提出了一个新的 GPU API，它指示命令处理器丢弃获取的内核并停止从设备队列 (DQ) 获取更多内核。基于此，DQ 可以通过软硬件协同设计主动重置，取代我们的纯软件解决方案（即延迟驱逐）。最后，GPU 运行时可以为开发人员提供高级 API 来重置 GPU 流，方法是丢弃内部数据结构（例如主机队列）中缓冲的内核并通过两个新 API 重置 GPU。我们相信这些扩展可以大大简化实现，甚至在闭源GPU上完全实现基于复位的抢占，并进一步提高性能，例如在10 μs内立即抢占GPU。</p>
<h3 id="有关任务"><a href="#有关任务" class="headerlink" title="有关任务"></a>有关任务</h3><h4 id="DNN-推理服务系统。"><a href="#DNN-推理服务系统。" class="headerlink" title="DNN 推理服务系统。"></a>DNN 推理服务系统。</h4><p>现有模型服务系统主要关注于满足服务级别目标（SLO），通常在数十毫秒内，并提高数据中心应用程序的整体吞吐量。 Clockwork [28] 利用 DNN 推理的延迟可预测性来实现低尾部延迟。它在专用 GPU 上顺序运行推理以提供可预测的性能。 Clipper [20] 和 Nexus [67] 支持在同一模型上进行批处理推理，以提高 GPU 利用率和推理吞吐量。 Abacus [22] 通过准确预测重叠算子的延迟来实现同步 DNN 推理。 INFaaS [64] 可以为每个推理自动选择具有不同优化的正确变体，以满足不同的 SLO。然而，<strong>数据中心应用程序的延迟 SLO 比实时系统的延迟 SLO 宽松得多，例如其单独运行延迟的 2 倍 [22]。因此，使用非抢占式调度或批处理方案对于数据中心应用程序有效，但对于实时场景（例如自动驾驶车辆）则无效</strong>。此外，REEF的设计与上述分布式服务系统是正交的。 REEF 中的两个关键机制也可以集成到其中，以提高每个 GPU 的吞吐量并保持实时推理的低延迟。 </p>
<p>GPU 内核抢占。除了软件抢占技术之外，先前的工作还提出了硬件增强来支持抢占式 GPU 调度[44,56,70]。一个直观的解决方案是支持 GPU 上的上下文切换 [70]。然而，由于上下文较大（例如，大量寄存器），GPU 上的成本远高于 CPU。珍等人。[44]提出了轻量级上下文切换以避免不必要的寄存器节省。塔纳西 ́ c 等人。 [70]扩展了硬件，通过停止发出新的线程块来被动抢占 GPU 的流式多处理器（SM）。 Chimera [56]进一步提出了SM刷新，以在检测到幂等执行时立即抢占SM。不同的是，我们的方法改进了现有的硬件机制，不需要对 GPU 进行修改即可实现即时抢占。 </p>
<p>GPU 多任务处理。人们已经做出了许多努力来同时执行多个 GPU 内核以实现高吞吐量 [27,43,55,57,74,76]。对于 DNN 计算，Rammer [47] 采用整体方法在编译时利用内核间和内核内并行性，它使用静态内核融合 [74] 来强制并发内核的 CU 分配。然而，静态内核融合要求融合内核在编译时已知，这不适用于REEF中的动态任务调度。R EEF提出动态内核填充以允许在运行时做出调度决策。先前的工作还提出了建模和预测并发内核执行速度减慢的方法[13,14,86,88]。 DASE [34] 对并发内核的内存争用进行建模。 Themis [87]使用神经网络来预测性能干扰。该预测可以帮助做出调度决策，以满足实时内核的延迟要求。然而，预测并不总是准确的，减速确实发生了。不同的是，REEF 中的动态内核填充强制并发内核仅使用实时内核剩余的 GPU 资源。目前，REEF主要关注GPU计算资源（即CU），并假设其他资源充足（例如GPU内存和带宽）。我们把它留作未来的工作。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>本文介绍了 REEF，这是第一个适用于商用 GPU 的 DNN 推理服务系统。它可以在GPU调度中实现微秒级的内核抢占和受控并发执行。首先，REEF 可以通过在微秒级主动终止和恢复尽力而为的内核，在 GPU 上启动实时内核。其次，REEF 可以使用适当的尽力而为内核动态填充实时内核，从而以可忽略的开销充分利用 GPU。此外，我们还为 DNN 推理服务构建了一个新的基准 (DISB)，其中包含不同的工作负载和真实世界的跟踪。使用 DISB 和微基准测试进行的评估证实了 REEF 在 AMD 和 NVIDIA GPU 上的功效和效率。</p>
<h2 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h2><h3 id="静态内核融合"><a href="#静态内核融合" class="headerlink" title="静态内核融合"></a>静态内核融合</h3><h3 id="忘掉的C-知识"><a href="#忘掉的C-知识" class="headerlink" title="忘掉的C++知识"></a>忘掉的C++知识</h3><p>内联函数</p>
<h3 id="内核-流-CU占用和寄存器数量"><a href="#内核-流-CU占用和寄存器数量" class="headerlink" title="内核\流\CU占用和寄存器数量"></a>内核\流\CU占用和寄存器数量</h3><h3 id="动态共享内存-什么过度分配-没懂"><a href="#动态共享内存-什么过度分配-没懂" class="headerlink" title="动态共享内存(什么过度分配?没懂)"></a>动态共享内存(什么过度分配?没懂)</h3>]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>Usher: Holistic Interference Avoidance for Resource Optimized ML Inference论文笔记</title>
    <url>/2024/10/25/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>随着深度学习模型的日益普及，最小化货币成本和最大化推理服务系统的吞吐量变得越来越重要。虽然希望对 GPU 资源进行空间复用以提高利用率，但现有技术受到模型间干扰的影响，这阻碍了它们同时实现高计算和内存利用率。我们推出了 USHER，这是一个能够以整体方式最大限度地提高资源利用率，同时具有干扰感知能力的系统。 USHER 由三个关键组件组成：1) 一个经济高效且快速的基于 GPU 内核的模型资源需求估计器，2) 一个轻量级的基于启发式的干扰感知资源利用率最大化调度程序，用于决定批量大小、模型复制程度和模型放置最小化货币成本，同时满足延迟 SLO 或最大化吞吐量，以及 3) 一种新颖的运算符图合并，用于合并类似权重的多个模型，以最大程度地减少 GPU 缓存中的干扰。使用生产工作负载的大规模实验表明，与现有方法相比，USHER 的吞吐量提高了 2.6 倍，成本效率提高了 3.5 倍，同时可扩展到数千个 GPU。</p>
<p>代码：<a href="https://github.com/ss7krd/Usher/">https://github.com/ss7krd/Usher/</a></p>
<span id="more"></span>

<h1 id="文章主体"><a href="#文章主体" class="headerlink" title="文章主体"></a>文章主体</h1><p>我们使用 Cuti 和 Muti 分别表示 GPU 计算和内存利用率，并使用 Creq 和 Mreq 表示模型对它们的要求。模型的 Creq（或 Mreq）是模型在执行期间的任何时刻消耗的 GPU 总计算（或内存）空间的最高百分比。我们进一步用Rreq来表示Mreq和Creq的和。我们使用 C-heavy 和 M-heavy 分别表示计算密集型和内存密集型。</p>
<h2 id="基本发现"><a href="#基本发现" class="headerlink" title="基本发现"></a>基本发现</h2><p>观察1.现有的推理服务系统无法最大化Cuti或Muti，并且它们的模型复用由于模型干扰而显着降低了吞吐量。此外，最大化 Cuti 并不一定最大化 Muti。（很多模型都是C重或是M重的）</p>
<p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029152349418.png" alt="image-20241029152349418"></p>
<p>观察2.在基于空间复用的推理服务中，与现有系统不同，即使一个 GPU 足以完成 SLO 内的工作负载，我们也可能需要划分模型的工作负载，以提高整体资源利用率。（图4）</p>
<p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029152848852.png" alt="image-20241029152848852"></p>
<p>观察 3. 不应为每个模型单独决定最佳工作负载划分。相反，同时考虑所有模型的整体方法至关重要。（参考图3）</p>
<p>一般概念假设大模型具有高 Creq 和 Mreq，而小模型具有低 Creq 和 Mreq。然而，这种区别忽略了 BS 在推理服务中的影响。增加 BS 可能会提高资源使用率，但存在延迟违规和内存溢出的风险。这凸显了 Cuti 和 Muti 在工作负载调度方面的微妙平衡。例如，当我们在 Nvidia H100 GPU 中以 BS&#x3D;4 执行 LlaMA-2（具有 130 亿个参数）时，它几乎占用了所有 GPU 内存，但有 45% 的 Cuti 未使用。进一步增加 BS 会导致内存溢出。因此，尽管它是一个大型模型，但它是 M 重而不是 C 重。另一方面，BS&#x3D;128 的 MobileNetV2（只有 340 万个参数）的 Cuti 高达 93%，但 Muti 为 30%。进一步增加 BS 会违反其 64 毫秒 SLO。因此，它是 C 重模型而不是 M 重模型。图 6a 显示了模型的 CDF 与模型的比率 Creq&#x2F;Mreq 的关系。我们看到 22% 的模型的比率≤0.75，表明它们是 M 重的。另外，28%的模型的比率在(1.35,1.65]，表明它们是Cheavy。Llama-2是M重模型(即Mreq&#x2F;Creq≥1.2)。在其他小模型中，39%是M -heavy，2% 具有可比的 Creq 和 Mreq，其余为 C-heavy（即 Creq&#x2F;Mreq≥1.2）。</p>
<p>观察4.与普遍看法不同，模型参数大小本身并不能决定模型是 Cheavy 还是 M-heavy。在 BS、依赖于 BS 的资源需求和 SLO 之间复杂关系的驱动下，即使是一个小模型也可以超越 Creq 中的较大模型。</p>
<p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029153551110.png" alt="image-20241029153551110"></p>
<p>观察 5. 将 C 重模型与 M 重模型复用会增加 GPU 的 Cuti 和 Muti。</p>
<p>给定 CNN 模型 A 和 B，我们首先找到两个模型中每个可能的卷积层对之间的权重相似性。然后，我们将所有层对的平均权重相似度作为两个模型之间的权重相似度。为了找到第 i 层和第 j 层之间的权重相似度，其中 Lx 表示模型 x 中所有卷积层的集合，我们计算了 2×|max(Wi A∩W j B )| |W i A|+|W j B| ，其中 max(W i A ∩W j B ) 表示权重矩阵 W i A 和 W j B 之间的最长公共子矩阵。如果两个权重值的绝对差异非常小，我们认为它们相同</p>
<p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029153748440.png" alt="image-20241029153748440"></p>
<p>观察 6. 不同 CNN 模型之间以及不同 Transformer 模型之间存在显着的权重重叠。</p>
<h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029153803154.png" alt="image-20241029153803154"></p>
<h3 id="基于-GPU-内核的资源需求估计器（GK-Estimator）。"><a href="#基于-GPU-内核的资源需求估计器（GK-Estimator）。" class="headerlink" title="基于 GPU 内核的资源需求估计器（GK-Estimator）。"></a>基于 GPU 内核的资源需求估计器（GK-Estimator）。</h3><p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029154303254.png" alt="image-20241029154303254"></p>
<p>（上面形成的二部图就是算子生成的图，边是输入输出的中介数据）</p>
<p>估计器根据给定的配置快速、正确地计算 GPU 类型中模型的 Creq 和 Mreq。</p>
<p>离线分析估计每个新模型的 Creq 和 Mreq 的常用方法。然而，这是昂贵且耗时的。为了应对这一挑战，我们提出了 GK-Estimator，它通过分析低级 GPU 内核来独立估计每个模型的资源需求，而无需在 GPU 中实际运行模型。我们以 Mreq 为例来解释 GK-Estimator 的工作原理。每个模型都可以被视为一个计算图，其中节点是算子，边是张量（即多维矩阵），表示模型输入或模型层生成的中间数据。在内部，每个算子执行都涉及顺序调用 GPU 编程框架中定义的一个或多个 GPU 内核 API（例如，用于 Nvidia GPU 的 CUDA、用于 AMD GPU 的 ROCm）。对于 ONNX 中定义的每个运算符，我们使用 Nvidia Profiling 工具发现在运算符执行期间 ML 框架调用了哪些 GPU 内核。我们注意到，分别有 2%、8%、56% 和 34% 的算子调用了 1、2、3 和 4 个 GPU 内核。</p>
<p>新模型的算子通常来自一组预先已知的算子 [53, 54]。因此，GK-Estimator 使用回归模型，根据输入张量的大小和算子的数学运算，快速计算算子生成的中间数据所需的内存。那么，对于顺序DL模型，其Mreq是模型参数大小和算子所需的最高内存之和。具有并行分支的模型的内存需求来自并发执行的内核的中间数据的Meqs。然后，如图 10 所示，首先，GK-Estimator 通过用其调用的 GPU 内核序列替换每个运算符，将运算符级计算图转换为内核级计算图。对于 ONNX 中的每个操作员，都会离线找到此序列。然后，它找到将同时执行的每组内核。接下来，对于每个集合，它通过回归模型（称为MreqRegressor）估计集合中每个内核生成的中间数据的Mreq，然后将集合中所有内核的Mreq求和。最后，将<strong>模型参数大小与所有集合中的最大内存需求之和</strong>作为模型的Mreq。</p>
<p>模型的第一个内核（即直接获取模型输入的内核）的启动时间为 0thms。为了确定哪些内核将同时执行，GK-Estimator 首先查找每个内核的开始时间。它使用另一个回归模型来估计每个内核的执行时间持续时间（称为时间回归器）。启动时间差不超过 τms（例如 0.001ms）的内核被视为可能同时运行。</p>
<p>使用一个集成学习回归器离线训练，可以达到98%的准确率。</p>
<h3 id="干扰感知资源利用率最大化调度程序（IR-调度程序）。"><a href="#干扰感知资源利用率最大化调度程序（IR-调度程序）。" class="headerlink" title="干扰感知资源利用率最大化调度程序（IR 调度程序）。"></a>干扰感知资源利用率最大化调度程序（IR 调度程序）。</h3><p>USHER 不是解决复杂度很高的优化问题，而是提供轻量级启发式方法来快速导出时间表。它首先以最大化在组内复用 C 重模型和 M 重模型的机会的方式对模型进行分组。然后，在每个组中，它选择能够实现特定目标（通过利用 O2-5）的最佳性能的配置。 IR-Scheduler 确保每个模型副本在其所在的 GPU 中获取其 Creq 和 Mreq，从而确保 C 空间和 M 空间中不存在模型间干扰。 </p>
<h4 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h4><p>基于O4和O5，将C重模型与M重模型复用可以最大化Ruti。这种多路复用使得 GPU 的 Cuti 和 Muti 具有可比性。如果 GPU 的 C 空间远低于其 M 空间，反之亦然，它可能无法托管额外的模型。</p>
<p>基于此，我们在复用模型时遵循一个原则。也就是说，模型的 Creq 之和几乎等于 GPU 中模型的 Mreq 之和（即 Σi Creqi ≈ Σi Mreqi）。在此基础上，USHER 对模型进行分组，使得每组中的模型 <strong>Σi Creqi ≈ Σi Mreqi</strong>。</p>
<p>在进行分组之前，USHER首先找到每个模型的Creq和Mreq。 USHER 使用 GK-Estiamtor（第 3.2 节中所述）计算所有可能的 BS 和 GPU 类型组合的平均 Rreq。对于 GPU 类型，USHER 在 Creq 或 Mreq 分别超过该类型的最大 C 空间或 M 空间的 BS 处停止。接下来，USHER 使用 <strong>k 均值聚类</strong>的变体进行分组 [56]。一开始，每个组都由一个模型组成。 USHER 将每两个模型之间的距离计算为 D &#x3D; | Σi Creqi − Σi Mreqi|。然后，它使用k-means算法将模型分为几组，其中每组由两个模型组成，使得D最小化，即每组的Σi Creqi ≈ Σi Mreqi。接下来，将每个组视为一个元素，USHER 执行算法的另一遍。此过程将前一遍创建的两个组合并为一个，并将每组中的模型数量增加两倍。因此，如果我们决定每组中最多有 2p（即 4）个模型，则需要执行 p 遍算法。最后，模型被分为几组，组的集合表示为 G &#x3D; {G1, G2, …, Gn}。（两两分组）</p>
<h4 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h4><p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029161702907.png" alt="image-20241029161702907"></p>
<p><img src="/./../images/Usher-Holistic-Interference-Avoidance-for-Resource-Optimized-ML-Inference%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241029161715260.png" alt="image-20241029161715260"></p>
<p>算法2比较难理解：放置算法如算法 2 所示。USHER 首先根据使用 GKEstimator 作为输入给出的配置计算每个 GPU 类型中每个模型的 Creq 和 Mreq（第 3-4 行）。然后进一步将模型组 Gi 中的模型分为 C 重模型和 M 重模型（第 7 行）。如果模型的平均 C-req&#x2F;M-req ≥ 1.2，则模型为 C 重；如果 M-req&#x2F;C-req ≥ 1.2，则模型为 M 重。接下来，USHER 按 Creq+Mreq 的降序对两个子组中的每一个进行排序（第 8 行）。之后，USHER 将两个子组中的每两个模型配对以创建 Final_model_list（第 9 行）。最后，USHER 将既不是 C 重也不是 M 重的模型插入到列表中，同时保持 Creq+Mreq 的降序。然后，USHER 从列表中逐一挑选一对或一个模型以分配给 GPU。</p>
<p>具体来说，USHER 调用 MODEL_REPLICA_PLACEMENT_WITHIN_GiGPU 函数（第 11 行）。该函数将尽可能多的 M 模型副本放置到该模式的 GPU 组（用 GiGPU 表示）中。模型组的 GPU 组定义为托管模型组的大部分模型副本的 GPU 组。基本上，当 USHER 为模型组 Gi 的任何模型副本初始化新 GPU 时，新 GPU 就会添加到 GPU 组 GiGPU 中。这样，USHER 会尝试将同一模型组中的模型副本放置到同一 GPU 组的 GPU 上。如果一个模型或一对模型有多个 GPU 可用，<strong>USHER 会选择托管后留下最低 C 空间 + M 空间的 GPU，以避免资源碎片。</strong></p>
<p>之后，USHER 调用 MODEL_REPLICA_PLACMENT_OUTSIDE_GiGPU，将尽可能多的 M 剩余模型副本放置到其他 GPU 组的 GPU 上（第 12 行）。最后，对于尚未放置到任何 GPU 的每个模型副本，USHER <strong>调用NEW_LOWEST_COST_GPU_INITIALIZATION 来初始化可以以最低成本托管模型或配对的 GPU 类型的新 GPU，并将 GPU 分配给 GiGPU</strong>（第 1315 行）。最后，布局算法将初始化新 GPU 的额外成本以及所有模型的总吞吐量返回给调度算法（第 19 行）。如[14]，模型的有效吞吐量被视为批量大小与第 954 届 USENIX 操作系统设计与实现 USENIX 协会完成批量的预期时间（包括队列等待时间）之间的比率。</p>
<h3 id="算子图合并（OG-Merger）。"><a href="#算子图合并（OG-Merger）。" class="headerlink" title="算子图合并（OG-Merger）。"></a>算子图合并（OG-Merger）。</h3><p>决定放置位置后，OG-Merger 会合并尽可能多的分配给 GPU 的模型的算子图，以最大限度地减少 GPU 缓存中的模型间干扰。合并后，合并后的图所分配的资源是分配给已合并图的模型（由 IR-Scheduler 决定）的资源总和。请注意，IR 调度程序不满足模型的缓存要求，我们发现在第 2 节的设置中几乎 100% 满足模型的缓存要求。因此，满足高速缓存要求会导致C空间和M空间的利用不足。这就是 USHER 在 OG-Merger 中单独解决缓存干扰的原因。</p>
<p>为了最大限度地减少 GPU 缓存的干扰，在分配给一个 GPU 的模型中，USHER 将尽可能多的模型运算符图合并到单个图中。为了执行最大算子图合并，USHER 首先根据模型的架构相似性（即结构和构成算子）将模型分组为子组。然后，对于每个子组，在 O6 的推动下，USHER 根据权重相似性决定多个图中要合并的运算符。最后，在合并过程中，USHER提取出需要合并的算子之间的最大公共权子矩阵，并确保同时处理不同模型的不同输入的与该子矩阵相关的矩阵乘法，而该子矩阵为在 GPU 缓存中。</p>
<h4 id="对类似架构的操作分组"><a href="#对类似架构的操作分组" class="headerlink" title="对类似架构的操作分组"></a>对类似架构的操作分组</h4><p>具有类似架构的算子图的模型的权重更有可能具有高度相似性。基于此，USHER 使用 DBSCAN 算法根据架构相似性对分配给 GPU 的模型进行分组。给定一组元素，DBSCAN 算法可以将元素之间距离很小（即 10−7）的元素聚集在同一组内，而不需要任何预定数量的组或组内的元素数量。在 DBSCAN 算法中，USHER 使用图编辑距离 来计算距离来衡量两个算子图之间的架构相似性。基本上，编辑距离算法发现需要执行多少操作符的添加&#x2F;删除&#x2F;替换才能使两个操作符图相同。距离越短意味着架构相似度越高。</p>
<h4 id="决定多个图中合并的运算符"><a href="#决定多个图中合并的运算符" class="headerlink" title="决定多个图中合并的运算符"></a>决定多个图中合并的运算符</h4><p> USHER 首先随机抽取两个模型。然后，它生成包含两个模型的运算符的二部图 B。当且仅当满足以下所有条件时，两个图的两个顶点之间存在边 e：(i) 相同类型（即卷积算子或注意力算子），(ii) 相同的起始时间（第 3.2 节中解释） ), (iii) 算子之间的权重相似度（第 2.4 节中描述）不小于 ω（例如 40%）。它被指定为边权重。生成 B 后，U SHER 在 B 中使用匈牙利算法 [59] 找到最大加权匹配，该算法选择一组独立的边（即不共享任何公共顶点），以使权重之和最大化。每条所选边的两个端点运算符都匹配并将被合并（第 3.4.2 节中进行了说明）。然后，USHER 从剩余模型中随机取出另一个模型，并通过重复相同的过程，使用 B 和另一个模型生成新的二分图 B’。重复此过程，直到组中不再有模型可以合并为止。</p>
<h4 id="执行合并操作"><a href="#执行合并操作" class="headerlink" title="执行合并操作"></a>执行合并操作</h4><p>作为运算符合并的示例，我们描述了两个 Transformer 模型的注意力层中两个 Query 运算符的过程。它是一个矩阵乘法运算：I′ 1 &#x3D; W1I1，其中I1是输入，W1是查询权重矩阵。现在，我们解释 USHER 如何修改此操作以进行运算符合并。如果 I1 和 I2 以及 W1 和 W2 的大小不同，我们将应用零填充以使它们大小相同。</p>
<p>只要加载一遍W1和W2重合的Ws，然后把矩阵乘法分配律用上之后分开：I′ 1 &#x3D; W sI1 + W R 1 I1. </p>
<h2 id="实验实践"><a href="#实验实践" class="headerlink" title="实验实践"></a>实验实践</h2><p>除了表1中描述的模型之外，我们还使用了两个包含多个DL模型的多模型应用程序：视频监控（SLO：500ms）[2]和社交媒体（SLO：750ms）[65] 。</p>
<p>除了具有稳定和密集的请求到达率的 Microsoft Azure Function Trace 2019 (MAF1) 之外，我们还尝试了具有突发到达率的 MAF Trace 2021 (MAF2) [66]。 (a) MAF1 (b) MAF2 图 13：固定集群的实际测试台中不同方法的良好吞吐量比较。 (a) MAF1 (b) MAF2 我们进行了真实测试台和模拟实验。真正的测试床是一个包含 6 个 AWS EC2 p3.8xlarge 服务器的集群，每个服务器由 4 个 V100 Nvidia GPU 组成，并且 GPU 通过 NVLink 互连。在模拟中，我们将GPU数量增加到6000个，以模拟大型企业级GPU集群[67]。由于使用如此多的 GPU 实际执行模型的成本极其昂贵，因此我们直接从调度程序决策中报告结果，而不实际运行模型。在模拟中，我们尝试了高达 15M 请求&#x2F;秒的超大工作负载，以模拟企业级集群中的大型工作负载。我们测试了固定和非固定集群设置。我们将 USHER 与 Shepherd [3]、GPUlet [14] 和 AlpaServe [4] 进行了比较。</p>
<p>主要实验包括：</p>
<ul>
<li><p>固定配置集群</p>
</li>
<li><p>非固定集群</p>
<ul>
<li>同质GPU</li>
<li>异构GPU</li>
</ul>
</li>
<li><p>overhead：对模型正确率的影响</p>
</li>
<li><p>限制SLO情况下</p>
<ul>
<li>固定集群</li>
<li>非固定集群</li>
</ul>
</li>
<li><p>不同工作负载下GPU可扩展性</p>
</li>
<li><p>消融实验</p>
</li>
<li><p>敏感性分析（w和t）</p>
</li>
</ul>
<p>结果略，反正表现很好就对了。</p>
<h1 id="术语积累"><a href="#术语积累" class="headerlink" title="术语积累"></a>术语积累</h1><h2 id="计算利用率和内存利用率"><a href="#计算利用率和内存利用率" class="headerlink" title="计算利用率和内存利用率"></a>计算利用率和内存利用率</h2><p>GPU 内存利用率（Muti）和计算利用率（Cuti）的需求主要由模型的任务类型和架构决定，不同类型的模型会对内存和计算资源提出不同的要求。</p>
<h3 id="1-内存利用率需求高的模型"><a href="#1-内存利用率需求高的模型" class="headerlink" title="1. 内存利用率需求高的模型"></a>1. 内存利用率需求高的模型</h3><p><strong>图像生成模型（如 GANs）</strong>和<strong>大规模预训练语言模型（如 GPT-3、BERT）</strong>通常对内存有很高需求。这类模型的参数量较大，尤其是预训练语言模型，它们需要存储大量的词嵌入和参数矩阵。因此，在推理或训练中，它们的内存需求通常较高。此外：</p>
<ul>
<li><strong>Transformer 模型</strong>（如 BERT 和 GPT）因其自注意力机制，需存储大量激活值和中间结果，导致内存需求增加。</li>
<li><strong>卷积神经网络（CNN）</strong>用于高分辨率图像处理时，也需要较高的内存来存储特征图。</li>
</ul>
<h3 id="2-计算利用率需求高的模型"><a href="#2-计算利用率需求高的模型" class="headerlink" title="2. 计算利用率需求高的模型"></a>2. 计算利用率需求高的模型</h3><p>**卷积神经网络（CNNs）**和**图像分类或对象检测模型**（如 ResNet、YOLO 等）通常是计算密集型。这些模型会进行大量矩阵乘法和卷积操作，因而对计算资源的需求较大。此外：</p>
<ul>
<li><strong>视频处理模型</strong>也有高计算需求，因为要处理每一帧图像，并可能涉及时间上的信息建模。</li>
<li><strong>Recurrent Neural Networks（RNNs）</strong>和 LSTM 也由于其时间依赖性操作（如在序列数据上的递归操作），在长序列数据上对计算资源需求较高。</li>
</ul>
<h3 id="3-GPU-的内存和计算利用率决定因素"><a href="#3-GPU-的内存和计算利用率决定因素" class="headerlink" title="3. GPU 的内存和计算利用率决定因素"></a>3. GPU 的内存和计算利用率决定因素</h3><p>GPU 的架构在很大程度上决定了内存利用率和计算利用率的表现，具体包括以下几点：</p>
<ul>
<li><strong>CUDA 核心数量和频率</strong>：决定了 GPU 可以进行的并行计算能力，从而影响计算利用率。</li>
<li><strong>内存带宽和显存大小</strong>：内存带宽越大，GPU 处理内存密集型任务的效率越高，显存大小也会影响大型模型是否能完全加载进 GPU。</li>
<li><strong>SM（Streaming Multiprocessor）数量和结构</strong>：这直接决定了每个 GPU 核心的处理能力和任务分配情况，进而影响计算和内存的综合利用率。</li>
<li><strong>缓存机制和内存管理</strong>：较高的缓存可以提升模型执行速度，减少对全局内存的依赖，提高总体利用效率。</li>
</ul>
<p>不同架构和模型的组合会直接影响 GPU 的计算和内存使用表现，因此在选择 GPU 时，需根据模型需求做出针对性选择。</p>
<h2 id="SLO"><a href="#SLO" class="headerlink" title="SLO"></a>SLO</h2><p>在 GPU 计算中，SLO（Service Level Objective，服务级别目标） 是用来定义 GPU 服务性能的目标标准。SLO 的设定主要是为了确保服务能够达到特定的性能要求，例如响应时间、延迟、吞吐量或资源利用率等，从而为任务提供稳定和高效的计算环境。对 GPU 来说，SLO 通常设定的目标会围绕延迟、计算时间、资源占用比（如计算或内存利用率）等，以满足用户的需求或应用的实时性要求</p>
<h2 id="BS和RD"><a href="#BS和RD" class="headerlink" title="BS和RD"></a>BS和RD</h2><p><strong>BS（Batch Size，批量大小）</strong>：指模型在 GPU 上执行时的批处理数量，即模型每次传入 GPU 的数据量大小。批量越大，通常能够更好地利用 GPU 计算资源，但也可能受到 GPU 内存的限制。</p>
<p><strong>RD（Replication Degree，复制度）</strong>：指模型的复制数量，以提高并发度。每个模型可以创建多份副本（replica），RD 就是副本的数量。增大 RD 值可以分散模型负载，提高任务完成速度，但需要更多的 GPU 资源。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo搭建笔记</title>
    <url>/2024/10/23/hexo%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>计算机网络论文笔记</title>
    <url>/2024/11/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="论文1-中文综述"><a href="#论文1-中文综述" class="headerlink" title="论文1-中文综述"></a>论文1-中文综述</h1><p>摘要：人工智能利用各种优化技术从海量训练样本中学习关键特征或知识，以提较高的质量量，这对训练训练方法提出了更高的要求。然而，传统单机训练无法满足存储与计算性因此，利用多个计算节点协同的分布式训练系统成为热点研究方向之一。本文首先阐述了单机训练面临的主要挑战。其次、分解析了分布训练系统系统预急需解决的三个关键问题（<strong>分区、通信、聚合</strong>），基于上述问题总结了一套训练系统的通用框架与四个核心组件。在此基础上，总结了基于随机随机陡峭下降算法的中心化与去中心化架构构研分支，并对各研研分支优化算法与应用进行综述.最后,提出了未来可能的研究方向.</p>
<h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><p>划分组件负责数据或模型的拆分任务，并将拆分后的子数据集或子模型部署到相应计算节点．优化组件提供各类优化算法供各计算节点调用．通信组件负责各计算节点间数据传输与信息同步．聚合组件负责将各计算节点产生的中间训练结果进行聚合 ，并输出训练任务的全局解 ．上述四个组件各司其职 ，协同完成训练任务.</p>
<p><img src="/./../images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20241120144347053.png" alt="image-20241120144347053"></p>
<h2 id="划分组件"><a href="#划分组件" class="headerlink" title="划分组件"></a>划分组件</h2><p>数据并行、模型并行、混合并行（和网络关系不大）</p>
<h3 id="通信组件"><a href="#通信组件" class="headerlink" title="通信组件"></a>通信组件</h3><p>分布式训练系统与传统单机训练的最主要差别在于利用多个计算节点间的协同合作加速完成训练任务,因此通信成为分布式训练系统不可或缺的环节.然而,由于硬件设备、网络带宽和传输速率等因素的影响,分布式训练系统计算节点间的通信往往成为瓶颈,严重制约了训练性能.在这种情况下,通信组件试图设计合理、高效的通信机制,减少通信开销.通信机制不仅要考虑硬件系统层面的限制约束还要兼顾软件算法层面的优化问题.本小节将从<strong>通信内容、通信拓扑以及通信同步方式</strong>三个方面介绍分布式训练系统的通信组件</p>
<ul>
<li>通信内容：和并行模式有关，数据并行通信内容包括模型参数及更新，模型并行分布内容为中间结果。主要研究包括对通信内容量化压缩、稀疏化压缩、低秩分解等压缩算法以及神经网络的参数选择算法。</li>
<li>通信拓扑：包括物理拓扑和逻辑拓扑，这篇主要讨论物理拓扑。包括中心化架构、去中心化架构（包括All-reduce机制和Gossip机制）</li>
<li>通信同步方式：单&#x2F;多线程同步、异步算法。</li>
<li>协议&#x2F;算法</li>
</ul>
<h3 id="优化组件"><a href="#优化组件" class="headerlink" title="优化组件"></a>优化组件</h3><p>和网络无关，主要介绍使用人工智能的优化组件。</p>
<h3 id="聚合组件"><a href="#聚合组件" class="headerlink" title="聚合组件"></a>聚合组件</h3><p>基于加和&#x2F;集成获取最终结果，和网络也没什么关系。</p>
<h2 id="并行优化算法"><a href="#并行优化算法" class="headerlink" title="并行优化算法"></a>并行优化算法</h2><h3 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h3><p>略</p>
<h3 id="中心化架构算法"><a href="#中心化架构算法" class="headerlink" title="中心化架构算法"></a>中心化架构算法</h3><ul>
<li>同步算法：梯度在各个服务器节点上的分布</li>
<li>异步算法：中各计算节点完成当 前轮次迭代训练后无需等待其它计算节点，因而提升了计算资源的利用率【文献[107]提出了一种新类型中心化训练算法ALADDIN.该算法在服务器节点与计算节点之间采用非便捷通信方式，解决服务器节点的通信瓶颈问题.ALADDIN算法设计了新型PS触发本地参数更新策略及Lazy全局参数更新策略，以缓解方差增加的问题。通过收敛性该算法在非凸目标函数下具有线性加速比.此外,5 1 1期王恩东等:环球训练系统及其优化算法综针对延时同步并行 SSP 算法静态阈值设置难题，文献[108]提出了一种自适应动态同步阈值算法 (DynamicStaleSynchronousParallel,DSSP). 算法在运行时自适应地调整各次迭代的同步阈值，以减少速度较快计算节点对全局模型参数同步的等待时间，从增加迭代吞吐量并加快收精简】</li>
<li>非&#x2F;共享内存的分布式异步算法：【文献[109]在非共享内存条件下研究了“星状”网络拓扑（中心化架构）的异步更新策略，并提出了AsySGCON算法】</li>
</ul>
<h3 id="去中心化架构算法"><a href="#去中心化架构算法" class="headerlink" title="去中心化架构算法"></a>去中心化架构算法</h3><p>在中心化架构架构中，中心节点的通信拥塞问题将随着计算节点数量的增加而愈发严重。不一样的的是，去中心化架构算法依赖于计算节点之间复杂的通信机制造以规避中心化架构中的通信拥塞.去中心化架结构算法按照通信方式可解读为同步算法和异步算法。</p>
<p>主要是参数在节点之间的通信。</p>
<h2 id="分布式训练优化算法"><a href="#分布式训练优化算法" class="headerlink" title="分布式训练优化算法"></a>分布式训练优化算法</h2><h3 id="面向异构环境"><a href="#面向异构环境" class="headerlink" title="面向异构环境"></a>面向异构环境</h3><p>由于计算备现呈现出多样化发展态势，研究所员逐步探索并研究了环境下的多元化训练策略[120122]。 文献[120]提出了一个异构感知的去中心化分布式训练方法。</p>
<h3 id="面向联邦学习"><a href="#面向联邦学习" class="headerlink" title="面向联邦学习"></a>面向联邦学习</h3><h3 id="面向大模型"><a href="#面向大模型" class="headerlink" title="面向大模型"></a>面向大模型</h3><p>感觉是内存方面的。</p>
<h1 id="论文2-A-Survey-on-Distributed-Machine-Learning"><a href="#论文2-A-Survey-on-Distributed-Machine-Learning" class="headerlink" title="论文2-A Survey on Distributed Machine Learning"></a>论文2-A Survey on Distributed Machine Learning</h1><h2 id="常见用于人工智能的分布式框架介绍"><a href="#常见用于人工智能的分布式框架介绍" class="headerlink" title="常见用于人工智能的分布式框架介绍"></a>常见用于人工智能的分布式框架介绍</h2><p>Apache Spark [168, 169] 等流行框架抓住了机器学习成为新兴工作负载的机会，现在提供优化的库（例如 MLlib [98]）。另一方面，最初设计为在单台机器上运行的专用机器学习库已开始获得在分布式环境中执行的支持。例如，流行的库 Keras [35] 接收到在 Google 的 Tensorflow [1] 和 Microsoft 的 CNTK [129] 上运行的后端。 Nvidia 通过集体通信库 (NCCL) [106] 扩展了他们的机器学习堆栈，该库最初设计用于支持同一台机器上的多个 GPU，但版本 2 引入了在多个节点上运行的能力 [76]。该生态系统的中心（图 4）是为分布式机器学习而原生构建的系统，并围绕特定算法和操作模型进行设计，例如分布式集成学习、并行同步随机梯度下降 (SGD) 或参数服务器。</p>
<h2 id="本地分布式机器学习系统"><a href="#本地分布式机器学习系统" class="headerlink" title="本地分布式机器学习系统"></a>本地分布式机器学习系统</h2><p>分布式集成学习，各种并行【学习】策略，不涉及底层网络架构和通信，但可能涉及组件组成的抽象网络。</p>
<p><strong>百度</strong>在此过程中包含了 Patarasuk 和 Yuan [118] 的进一步优化，称为 Ring AllReduce，以减少所需的通信量。通过将机器集群构建为环（每个节点只有两个邻居）并级联归约操作，可以最佳地利用所有带宽。那么，瓶颈就是相邻节点之间的最高延迟。百度声称在应用该技术训练深度学习网络时可以实现线性加速。然而，它仅在相对较小的集群上进行了演示（每个节点有五个节点，尽管每个节点都有多个 GPU，通过同一系统相互通信）。默认情况下，该方法缺乏容错能力，因为环中的任何节点都不会被遗漏。这可以通过冗余来抵消（以效率为代价）。然而，如果不这样做，则该方法的可扩展性受到所有节点可用的概率的限制。当使用大量商用机器和网络时，这种概率可能很低，而这是促进大数据发展所必需的。百度的系统已集成到 Tensorflow 中，作为基于内置参数服务器的方法（如下所述）的替代方案。</p>
<p><strong>Horovod</strong> [131]采用了与百度非常相似的方法：它向 Tensorflow 添加了一层基于 AllReduce 的 MPI 训练。一个区别是，Horovod 使用 NVIDIA 集体通信库 (NCCL) 来提高在 (Nvidia) GPU 上训练时的效率。这还允许在单个节点上使用多个 GPU。对现有 Tensorflow 模型进行数据并行化相对简单，因为只需要添加几行代码，将默认的 Tensorflow 训练例程包装在分布式 AllReduce 操作中。当使用 128 个 GPU 对 Inception v4 [148] 和 ResNet-101 [68] 进行基准测试时，平均 GPU 利用率约为 88%，而 Tensorflow 参数服务器方法中的平均 GPU 利用率约为 50%。然而，Horovod 缺乏容错能力（就像百度的方法一样），因此遇到了同样的可扩展性问题 [53]。</p>
<p>……etc 如何通过reduce方法减少延迟</p>
<p>一些算法优化……</p>
<h1 id="A-Survey-on-Trusted-Distributed-Artificial-Intelligence"><a href="#A-Survey-on-Trusted-Distributed-Artificial-Intelligence" class="headerlink" title="A Survey on Trusted Distributed Artificial Intelligence"></a>A Survey on Trusted Distributed Artificial Intelligence</h1><p>新兴的人工智能 (AI) 系统正在彻底改变计算和数据处理方法，对社会产生巨大影响。数据通过自动标记管道进行处理，而不是将其作为系统的输入提供。创新性提高了监控&#x2F;检测&#x2F;反应机制的整体性能，以实现高效的系统资源管理。然而，由于硬件驱动的设计限制，网络和信任机制不够灵活和自适应，无法动态交互和控制资源。新颖的自适应软件驱动设计方法可以使我们通过虚拟化具有最大化功能的网络功能来构建具有软件定义网络（SDN）功能的不断增长的智能机制。这些挑战和关键功能集已被确定，并以其人工智能系统的科学背景和不断发展的智能机制引入到本次调查中。此外，还探讨和讨论了 1950-2021 年间的障碍和研究挑战，重点关注近年来的情况。这些挑战根据新兴可信分布式人工智能机制的三个定义的架构视角（集中式、分散式&#x2F;自主式、分布式&#x2F;混合式）进行分类。因此，可以通过端到端可信执行环境（TEE）在动态环境中确保弹性和稳健性，以促进智能机制和系统的发展。此外，正如论文中所提出的，基于可信分布式人工智能（TDAI）的信任测量、量化和证明方法可以应用于新兴的分布式系统及其底层的多样化应用领域，这将在我们未来的相关领域中进行探索和实验。</p>
<p>【看不懂，但是是偏狭义的网络，以及一个基于人工智能的分布式网络设计……】</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2024/10/21/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
